@book{Laster1998,
abstract = {This work presents a modular approach to temporal logic model checking of software.},
author = {Laster, Karen and Grumberg, Orna},
doi = {10.1007/BFb0054162},
file = {:home/arc/Codes/PythonDLCourse/machinelearninginaction/Machine Learning in Action.pdf:pdf},
isbn = {978-3-540-69753-4},
issn = {16113349},
pages = {20--35},
title = {{ML python}},
url = {http://link.springer.com/10.1007/BFb0054162},
year = {1998}
}
@misc{,
file = {:home/arc/Codes/PythonDLCourse/machinelearninginaction/机器学习实战.pdf:pdf},
title = {{机器学习实战{\%}28Peter Harrington 著{\%}29.pdf}}
}
@article{Shi2015a,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
archivePrefix = {arXiv},
arxivId = {1506.04214},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
eprint = {1506.04214},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Convolutional LSTM Network$\backslash$: A Machine Learning
Approach for Precipitation Nowcasting.pdf:pdf},
pages = {1--12},
title = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
url = {http://arxiv.org/abs/1506.04214},
year = {2015}
}
@article{He2016a,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.03385v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {arXiv:1512.03385v1},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Deep Residual Learning For Image Recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
volume = {2016-December},
year = {2016}
}
@article{Szegedy2015b,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.4842v1},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {arXiv:1409.4842v1},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
title = {{Going deeper with convolutions}},
volume = {07-12-June-2015},
year = {2015}
}
@article{He2016b,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.05027v3},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {arXiv:1603.05027v3},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Identity Mappings in Deep Residual Networks.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
volume = {9908 LNCS},
year = {2016}
}
@article{Monien2007,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
author = {Monien, Burkhard and Preis, Robert and Schamberger, Stefan},
doi = {10.1201/9781420010749},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781420010749},
journal = {Handbook of Approximation Algorithms and Metaheuristics},
pages = {60--1--60--16},
title = {{Approximation algorithms for multilevel graph partitioning}},
year = {2007}
}
@article{Szegedy2016a,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
eprint = {1602.07261},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Inception-v4, Inception-ResNet and
the Impact of Residual Connections on Learning.pdf:pdf},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{Lin2013a,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Network In Network.pdf:pdf},
pages = {1--10},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Szegedy2016b,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.00567v3},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {arXiv:1512.00567v3},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
volume = {2016-December},
year = {2016}
}
@article{Simonyan2014a,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Very Deep Convolutional Networks For Large-Scale Image Recognition.pdf:pdf},
pages = {1--14},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Zeiler2014a,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1311.2901v3},
file = {:home/arc/Codes/PythonDLCourse/References/CNN/Visualizing and Understanding Convolutional Networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@article{Maire,
author = {Maire, Michael and Fowlkes, Charless and Malik, Jitendra},
file = {:home/arc/Codes/PythonDLCourse/References/cv/Contour Detection and
Hierarchical Image Segmentation.pdf:pdf},
pages = {1--20},
title = {{Amfm{\_}Pami2011}}
}
@article{Hadidi2014a,
abstract = {Deep Neural Networks (DNNs) have recently shown outstanding performance on the task of whole image classification. In this paper we go one step further and address the problem of object detection -- not only classifying but also precisely localizing objects of various classes using DNNs. We present a simple and yet powerful formulation of object detection as a regression to object masks. We define a multi-scale inference procedure which is able to produce a high-resolution object detection at a low cost by a few network applications. The approach achieves state-of-the-art performance on Pascal 2007 VOC.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.2249v1},
author = {Hadidi, Niloufar Niakosari and Cullen, Kathryn R. and Hall, Leah M.J. and Lindquist, Ruth and Buckwalter, Kathleen C. and Mathews, Emily},
doi = {10.3928/19404921-20140820-01},
eprint = {arXiv:1312.2249v1},
file = {:home/arc/Codes/PythonDLCourse/References/cv/deep-neural-networks-for-object-detection.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {1940-4921},
journal = {Research in Gerontological Nursing},
number = {5},
pages = {200--205},
pmid = {25157535},
title = {{DetectorNet}},
url = {http://www.healio.com/doiresolver?doi=10.3928/19404921-20140820-01},
volume = {7},
year = {2014}
}
@article{Kuo2015a,
abstract = {Existing object proposal approaches use primarily bottom-up cues to rank proposals, while we believe that objectness is in fact a high level construct. We argue for a data-driven, semantic approach for ranking object proposals. Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method. We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster. We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP. Our implementation achieves this performance while running at 260 ms per image.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.02146v2},
author = {Kuo, Weicheng and Hariharan, Bharath and Malik, Jitendra},
doi = {10.1109/ICCV.2015.285},
eprint = {arXiv:1505.02146v2},
file = {:home/arc/Codes/PythonDLCourse/References/cv/DeepBox$\backslash$: Learning Objectness with Convolutional Networks.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2479--2487},
title = {{DeepBox: Learning objectness with convolutional networks}},
volume = {2015 International Conference on Computer Vision, ICCV 2015},
year = {2015}
}
@article{Lowe2004a,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of affine distortion , change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G},
file = {:home/arc/Codes/PythonDLCourse/References/cv/Distinctive image features from scale-invariant keypoints.pdf:pdf},
journal = {International Journal of Computer Vision},
pages = {1--28},
title = {{ISSCR{\%}20JA1{\%}20OL.pdf}},
year = {2004}
}
@article{Zitnick2013,
abstract = {The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box's boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96{\%} object recall at overlap threshold of 0.5 and over 75{\%} recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
author = {Zitnick, C Lawrence and Doll, Piotr},
file = {:home/arc/Codes/PythonDLCourse/References/cv/Edge Boxes$\backslash$: Locating Object Proposals from Edges.pdf:pdf},
keywords = {edge detection,object detection,object proposals},
title = {{Edge Boxes: Locating Object Proposals from Edges - Microsoft Research}},
url = {https://pdollar.github.io/files/papers/ZitnickDollarECCV14edgeBoxes.pdf{\%}0Ahttp://research.microsoft.com/apps/pubs/default.aspx?id=220569},
year = {2013}
}
@article{Arabyat2014,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. 1},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Arabyat, Yaser},
doi = {10.1109/CVPR.2005.177},
eprint = {9411012},
file = {:home/arc/Codes/PythonDLCourse/References/cv/Histograms of Oriented Gradients for Human Detection.pdf:pdf},
isbn = {0769523722},
issn = {1063-6919},
journal = {Journal of Finance and Accounting},
keywords = {a result of the,banking sector,banking system,century the world has,changes and improvements as,field of information technology,i,information technology,it,it bank system,s,since the end of,the beginning of this,the twentieth century and,tremendous advances in the,witnessed large-scale},
number = {8},
pages = {167--175},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{Towards Improving Efficiency in Banking Sector using Information Technology}},
volume = {5},
year = {2014}
}
@article{Kenkkila2010,
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Kenkkil{\"{a}}, Jussi},
doi = {10.1109/ICCV.2011.6126456},
eprint = {1409.4842},
file = {:home/arc/Codes/PythonDLCourse/References/cv/Selective search for object recognition.pdf:pdf},
isbn = {9781457711015},
issn = {1573-1405},
journal = {Image (Rochester, N.Y.)},
pages = {1--8},
pmid = {24920543},
title = {{BIU 1st Imaging Workshop : Practical Microscopy and Image Light microscopy Why an imaging course ? Light microscopy applications Transmission of light thru media}},
year = {2010}
}
@article{Radford2015a,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {1511.06434},
file = {:home/arc/Codes/PythonDLCourse/References/DCGAN/UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS.pdf:pdf},
pages = {1--16},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Lin2017b,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.03144v2},
author = {Lin, Tsung Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
doi = {10.1109/CVPR.2017.106},
eprint = {arXiv:1612.03144v2},
file = {:home/arc/Codes/PythonDLCourse/References/FPN/Feature Pyramid Networks for Object Detection.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {936--944},
title = {{Feature pyramid networks for object detection}},
volume = {2017-January},
year = {2017}
}
@article{Moravcik2017a,
abstract = {Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.01724v1},
author = {Moravcik, Matej and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
doi = {10.1126/science.aam6960.1},
eprint = {arXiv:1701.01724v1},
file = {:home/arc/Codes/PythonDLCourse/References/Game/DeepStack$\backslash$: Expert-Level Artificial Intelligence in
Heads-Up No-Limit Poker.pdf:pdf},
journal = {Science},
number = {May},
pages = {1--32},
title = {{DeepStack : Expert-Level Artificial Intelligence in No-Limit Poker}},
url = {http://science.sciencemag.org/},
volume = {513},
year = {2017}
}
@article{Mnih2015a,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/arc/Codes/PythonDLCourse/References/Game/Human-level control through deep reinforcement
learning.pdf:pdf},
title = {learning},
year = {2015}
}
@article{Silver2016a,
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Driessche, George Van Den and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray},
doi = {10.1038/nature16961},
file = {:home/arc/Codes/PythonDLCourse/References/Game/Mastering the game of Go with deep
neural networks and tree search.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7585},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Goodfellow,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian J and Pouget-abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-farley, David},
eprint = {arXiv:1406.2661v1},
file = {:home/arc/Codes/PythonDLCourse/References/GAN/Generative Adversarial Nets.pdf:pdf},
pages = {1--9},
title = {{Generative Adversarial Nets}}
}
@article{Auto-encoder,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.09027v1},
author = {Auto-encoder, Data},
eprint = {arXiv:1905.09027v1},
file = {:home/arc/Codes/PythonDLCourse/References/GAN/Learning to Confuse$\backslash$: Generating Training Time Adversarial
Data with Auto-Encoder.pdf:pdf},
title = {{Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder}}
}
@article{Wu2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1901.00596v2},
author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Member, Senior and Yu, Philip S},
eprint = {arXiv:1901.00596v2},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/A Comprehensive Survey on Graph Neural
Networks.pdf:pdf},
number = {X},
pages = {1--22},
title = {{A Comprehensive Survey on Graph Neural Networks}},
volume = {X},
year = {2018}
}
@article{Luxburg2007,
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {Luxburg, Ulrike Von},
eprint = {arXiv:0711.0189v1},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/clustering/A Tutorial on Spectral Clustering.pdf:pdf},
keywords = {graph laplacian,spectral clustering},
pages = {1--32},
title = {{A Tutorial on Spectral Clustering}},
year = {2007}
}
@article{Kushnir2006a,
author = {Kushnir, Dan and Galun, Meirav and Brandt, Achi},
doi = {10.1016/j.patcog.2006.04.007},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/clustering/Fast multiscale clustering and manifold identification.pdf:pdf},
keywords = {aggregation,algebraic multigrid,amg,astrophysical models,data analysis,graph partitioning,manifold,similarity-based clustering},
pages = {1876--1891},
title = {{Fast multiscale clustering and manifold identification}},
volume = {39},
year = {2006}
}
@article{,
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/clustering/METIS  Unstructured Graph Partitioning and Sparse Matrix Ordering System.pdf:pdf},
title = {{I S ∗}},
year = {1995}
}
@article{Dhillon2007a,
author = {Dhillon, Inderjit S and Guan, Yuqiang and Kulis, Brian},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/clustering/Weighted Graph Cuts without Eigenvectors$\backslash$: A Multilevel Approach.pdf:pdf},
number = {11},
pages = {1944--1957},
title = {{Weighted Graph Cuts without Eigenvectors : A Multilevel Approach}},
volume = {29},
year = {2007}
}
@article{Chen,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.04908v2},
author = {Chen, Meihao and Lin, Zhuoru},
eprint = {arXiv:1710.04908v2},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/Graph Convolutional Networks for Classification
with a Structured Label Space.pdf:pdf},
title = {{Graph Convolutional Networks for Classification with a Structured Label Space}}
}
@article{Li2019a,
archivePrefix = {arXiv},
arxivId = {arXiv:1904.12787v1},
author = {Li, Yujia and Gu, Chenjie and Dullien, Thomas and Vinyals, Oriol and Kohli, Pushmeet},
eprint = {arXiv:1904.12787v1},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/Graph Matching Networks for
Learning the Similarity of Graph Structured Objects.pdf:pdf},
title = {{Graph Matching Networks for Learning the Similarity of Graph Structured Objects}},
year = {2019}
}
@article{Tang,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.03578v1},
author = {Tang, Jian and Qu, Meng},
eprint = {arXiv:1503.03578v1},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/LINE$\backslash$: Large-scale Information Network Embedding.pdf:pdf},
isbn = {9781450334693},
keywords = {dimension reduction,feature learn-,information network embedding,ing,scalability},
title = {{LINE : Large-scale Information Network Embedding}}
}
@article{Hamrick,
archivePrefix = {arXiv},
arxivId = {arXiv:1806.01261v3},
author = {Hamrick, Jessica B and Bapst, Victor and Sanchez-gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt},
eprint = {arXiv:1806.01261v3},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/Relational inductive biases, deep learning, and graph networks.pdf:pdf},
pages = {1--40},
title = {{Relational inductive biases , deep learning , and graph networks}}
}
@article{Zhang2018a,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.07294v1},
author = {Zhang, Jiani and Shi, Xingjian and Chinese, The and Kong, Hong and Kong, Hong},
eprint = {arXiv:1803.07294v1},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spatial-based/GaAN$\backslash$: Gated Attention Networks for
Learning on Large and Spatiotemporal Graphs.pdf:pdf},
title = {{GaAN : Gated Attention Networks for Learning on Large and Spatiotemporal Graphs}},
year = {2018}
}
@article{Monti,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.08402v3},
author = {Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodol, Emanuele and Svoboda, Jan and Bronstein, Michael M},
eprint = {arXiv:1611.08402v3},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spatial-based/Geometric deep learning on graphs and manifolds using mixture model CNNs.pdf:pdf},
title = {{Geometric deep learning on graphs and manifolds using mixture model CNNs}}
}
@article{Romero2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.10903v3},
author = {Romero, Adriana},
eprint = {arXiv:1710.10903v3},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spatial-based/GRAPH ATTENTION NETWORKS.pdf:pdf},
pages = {1--12},
title = {{Raph ttention etworks}},
year = {2018}
}
@article{Hamilton2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.02216v4},
author = {Hamilton, William L},
eprint = {arXiv:1706.02216v4},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spatial-based/Inductive Representation Learning on Large Graphs.pdf:pdf},
number = {Nips},
pages = {1--19},
title = {{Inductive Representation Learning on Large Graphs}},
year = {2017}
}
@article{Wang,
archivePrefix = {arXiv},
arxivId = {arXiv:1808.03965v1},
author = {Wang, Zhengyang},
eprint = {arXiv:1808.03965v1},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spatial-based/Large-Scale Learnable Graph Convolutional Networks.pdf:pdf},
isbn = {9781450355520},
keywords = {deep learning,graph convolutional networks,graph mining,large-},
title = {{Large-Scale Learnable Graph Convolutional Networks}}
}
@article{Niepert2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.05273v4},
author = {Niepert, Mathias and Kutzkov, Konstantin and Kutzkov, Konstantin and Eu, Neclab},
eprint = {arXiv:1605.05273v4},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spatial-based/Learning Convolutional Neural Networks for Graphs.pdf:pdf},
title = {{Learning Convolutional Neural Networks for Graphs}},
volume = {1},
year = {2015}
}
@article{Dai2018a,
author = {Dai, Hanjun and Kozareva, Zornitsa and Dai, Bo and Smola, Alexander J and Song, Le},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spatial-based/Learning Steady-States of Iterative Algorithms over Graphs.pdf:pdf},
title = {{Learning Steady-States of Iterative Algorithms over Graphs}},
year = {2018}
}
@article{Li2018b,
archivePrefix = {arXiv},
arxivId = {arXiv:1801.03226v1},
author = {Li, Ruoyu and Wang, Sheng and Zhu, Feiyun and Huang, Junzhou},
eprint = {arXiv:1801.03226v1},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spectral-based/Adaptive Graph Convolutional Neural Networks.pdf:pdf},
title = {{Adaptive Graph Convolutional Neural Networks}},
year = {2018}
}
@article{Levie,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.07664v2},
author = {Levie, Ron and Monti, Federico and Bresson, Xavier and Bronstein, Michael M},
eprint = {arXiv:1705.07664v2},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spectral-based/CayleyNets$\backslash$: Graph Convolutional Neural Networks
with Complex Rational Spectral Filters.pdf:pdf},
pages = {1--20},
title = {{CayleyNets : Graph Convolutional Neural Networks with Complex Rational Spectral Filters}}
}
@article{Bresson2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1606.09375v3},
author = {Bresson, Xavier and Vandergheynst, Pierre},
eprint = {arXiv:1606.09375v3},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spectral-based/Convolutional Neural Networks on Graphs
with Fast Localized Spectral Filtering.pdf:pdf},
number = {Nips},
title = {{Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}},
year = {2016}
}
@article{Henaff,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.05163v1},
author = {Henaff, Mikael and Bruna, Joan and Lecun, Yann},
eprint = {arXiv:1506.05163v1},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spectral-based/Deep Convolutional Networks on Graph-Structured Data.pdf:pdf},
pages = {1--10},
title = {{Deep Convolutional Networks on Graph-Structured Data}}
}
@article{Kipf2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.02907v4},
author = {Kipf, Thomas N and Welling, Max},
eprint = {arXiv:1609.02907v4},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spectral-based/SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS.pdf:pdf},
pages = {1--14},
title = {{S -s c g c n}},
year = {2017}
}
@article{Bruna,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6203v3},
author = {Bruna, Joan and Szlam, Arthur},
eprint = {arXiv:1312.6203v3},
file = {:home/arc/Codes/PythonDLCourse/References/GraphDL/spectral-based/Spectral Networks and Deep Locally Connected Networks on Graphs.pdf:pdf},
pages = {1--14},
title = {{Spectral Networks and Deep Locally Connected Networks on Graphs arXiv : 1312 . 6203v3 [ cs . LG ] 21 May 2014}}
}
@article{Ruder2016a,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.04747v2},
author = {Ruder, Sebastian},
eprint = {arXiv:1609.04747v2},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/An overview of gradient descent optimizationalgorithms.pdf:pdf},
pages = {1--14},
title = {{An overview of gradient descent optimization}},
year = {2016}
}
@misc{,
file = {:home/arc/Codes/PythonDLCourse/References/Misc/backprop{\_}old.pdf:pdf},
title = {backprop{\_}old.pdf}
}
@misc{,
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Backpropagation Applied to Handwritten Zip Code Recognition.pdf:pdf},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition.pdf}}
}
@article{Ioffe,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03167v3},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {arXiv:1502.03167v3},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Batch Normalization$\backslash$: Accelerating Deep Network Training by
Reducing Internal Covariate Shift.pdf:pdf},
title = {{Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift}}
}
@article{,
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
title = {{A B7CEDGF HIB7PRQTSUDGQICWVYX HIB edCdSISIXvg5r ` CdQTw XvefCdS}}
}
@article{Shirish2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.07628v1},
author = {Shirish, Nitish and Richard, Keskar},
eprint = {arXiv:1712.07628v1},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Improving Generalization Performance by Switching from Adam to SGD.pdf:pdf},
number = {1},
title = {{Improving Generalization Performance by Switching from Adam to SGD}},
year = {2017}
}
@article{Deana,
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Large scale distributed deep networks.pdf:pdf},
pages = {1--11},
title = {{Large Scale Distributed Deep Networks}}
}
@article{Liu,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.07853v1},
author = {Liu, Xingyu and Lee, Joon-young},
eprint = {arXiv:1905.07853v1},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Learning Video Representations from Correspondence Proposals.pdf:pdf},
title = {{Learning Video Representations from Correspondence Proposals}}
}
@article{Berthelota,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.02249v1},
author = {Berthelot, David and Oliver, Avital and Carlini, Nicholas and Goodfellow, Ian and Raffel, Colin and Papernot, Nicolas},
eprint = {arXiv:1905.02249v1},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/MixMatch$\backslash$: A Holistic Approach to
Semi-Supervised Learning.pdf:pdf},
title = {{MixMatch : A Holistic Approach to Semi-Supervised Learning arXiv : 1905 . 02249v1 [ cs . LG ] 6 May 2019}}
}
@article{Sermanet,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6229v4},
author = {Sermanet, Pierre and Eigen, David},
eprint = {arXiv:1312.6229v4},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/OverFeat$\backslash$:Integrated Recognition, Localization and Detectionusing Convolutional Networks.pdf:pdf},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks arXiv : 1312 . 6229v4 [ cs . CV ] 24 Feb 2014}}
}
@article{Erhan,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.2249v1},
author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
eprint = {arXiv:1312.2249v1},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Scalable Object Detection using Deep Neural Networks.pdf:pdf},
title = {{Scalable Object Detection using Deep Neural Networks}}
}
@article{Carbin2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1803.03635v5},
author = {Carbin, Michael},
eprint = {arXiv:1803.03635v5},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/THE LOTTERY TICKET HYPOTHESIS $\backslash$: FINDING SPARSE , TRAINABLE NEURAL NETWORKS.pdf:pdf},
pages = {1--42},
title = {{T HE L OTTERY T ICKET H YPOTHESIS :}},
year = {2019}
}
@article{Glorot2010,
author = {Glorot, Xavier and Bengio, Yoshua},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
volume = {9},
year = {2010}
}
@article{Xie2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1904.12848v1},
author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-thang and Le, Quoc V and Brain, Google},
eprint = {arXiv:1904.12848v1},
file = {:home/arc/Codes/PythonDLCourse/References/Misc/Unsupervised Data Augmentation.pdf:pdf},
pages = {1--15},
title = {{Unsupervised Data Augmentation}},
year = {2018}
}
@article{Li,
author = {Li, Haoxiang and Lin, Zhe and Shen, Xiaohui and Brandt, Jonathan},
file = {:home/arc/Codes/PythonDLCourse/References/MTCNN/A Convolutional Neural Network Cascade for Face Detection.pdf:pdf},
title = {{A Convolutional Neural Network Cascade for Face Detection}}
}
@article{Yang,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06451v1},
author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
eprint = {arXiv:1509.06451v1},
file = {:home/arc/Codes/PythonDLCourse/References/MTCNN/From Facial Parts Responses to Face Detection$\backslash$: A Deep Learning Approach.pdf:pdf},
number = {3},
title = {{From Facial Parts Responses to Face Detection: A Deep Learning Approach}}
}
@article{Zhang,
author = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Member, Senior and Qiao, Yu and Member, Senior},
file = {:home/arc/Codes/PythonDLCourse/References/MTCNN/Joint Face Detection and Alignment using
Multi-task Cascaded Convolutional Networks.pdf:pdf},
number = {1},
pages = {1--5},
title = {{Joint Face Detection and Alignment using Multi - task Cascaded Convolutional Networks}}
}
@article{Viola2001,
author = {Viola, Paul and Jones, Michael},
file = {:home/arc/Codes/PythonDLCourse/References/MTCNN/Robust real-time face detection.pdf:pdf},
pages = {1--25},
title = {{Robust Real-time Object Detection}},
year = {2001}
}
@article{Zoph,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.07012v4},
author = {Zoph, Barret and Shlens, Jonathon},
eprint = {arXiv:1707.07012v4},
file = {:home/arc/Codes/PythonDLCourse/References/NAS/Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
title = {{Learning Transferable Architectures for Scalable Image Recognition}}
}
@article{Pang,
archivePrefix = {arXiv},
arxivId = {arXiv:1904.07392v1},
author = {Pang, Tsung-yi Lin Ruoming and Le, Quoc V},
eprint = {arXiv:1904.07392v1},
file = {:home/arc/Codes/PythonDLCourse/References/NAS/NAS-FPN$\backslash$: Learning Scalable Feature Pyramid Architecture for Object Detection.pdf:pdf},
title = {{NAS-FPN : Learning Scalable Feature Pyramid Architecture}}
}
@article{Zoph2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01578v2},
author = {Zoph, Barret and Le, Quoc V},
eprint = {arXiv:1611.01578v2},
file = {:home/arc/Codes/PythonDLCourse/References/NAS/NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING.pdf:pdf},
pages = {1--16},
title = {{N EURAL A RCHITECTURE S EARCH WITH}},
year = {2017}
}
@article{Anumanchipalli,
author = {Anumanchipalli, Gopala K and Chartier, Josh and Chang, Edward F},
doi = {10.1038/s41586-019-1119-1},
file = {:home/arc/Codes/PythonDLCourse/References/Neural Science/Speech synthesis from neural decoding of spoken sentences.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
publisher = {Springer US},
title = {{Article Speech synthesis from neural decoding of spoken sentences}},
url = {http://dx.doi.org/10.1038/s41586-019-1119-1}
}
@article{Hashimoto2016a,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01587v5},
author = {Hashimoto, Kazuma and Xiong, Caiming},
eprint = {arXiv:1611.01587v5},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/A Joint Many-Task Model$\backslash$:
Growing a Neural Network for Multiple NLP Tasks.pdf:pdf},
title = {{A Joint Many-Task Model :}},
year = {2016}
}
@article{Huang2019,
abstract = {This paper focuses on YOLO-LITE, a real-time object detection model developed to run on portable devices such as a laptop or cellphone lacking a Graphics Processing Unit (GPU). The model was first trained on the PASCAL VOC dataset then on the COCO dataset, achieving a mAP of 33.81{\%} and 12.26{\%} respectively. YOLO-LITE runs at about 21 FPS on a non-GPU computer and 10 FPS after implemented onto a website with only 7 layers and 482 million FLOPS. This speed is 3.8x faster than the fastest state of art model, SSD MobilenetvI. Based on the original object detection algorithm YOLOV2, YOLO- LITE was designed to create a smaller, faster, and more efficient model increasing the accessibility of real-time object detection to a variety of devices.},
archivePrefix = {arXiv},
arxivId = {arXiv:1811.05588v1},
author = {Huang, Rachel and Pedoeem, Jonathan and Chen, Cuixian},
doi = {10.1109/BigData.2018.8621865},
eprint = {arXiv:1811.05588v1},
file = {:home/arc/Codes/PythonDLCourse/References/Yolo/YOLO-LITE$\backslash$: A Real-Time Object Detection
Algorithm Optimized for Non-GPU Computers.pdf:pdf},
isbn = {9781538650356},
journal = {Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018},
keywords = {YOLO,deep learning,mobile,neural networks,non-GPU,object detection},
pages = {2503--2510},
title = {{YOLO-LITE: A Real-Time Object Detection Algorithm Optimized for Non-GPU Computers}},
year = {2019}
}
@article{Levie2019,
abstract = {The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach, in comparison to other spectral domain convolutional architectures, on spectral image classification, community detection, vertex classification and matrix completion tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.07664v2},
author = {Levie, Ron and Monti, Federico and Bresson, Xavier and Bronstein, Michael M},
doi = {10.1109/TSP.2018.2879624},
eprint = {arXiv:1705.07664v2},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Geometric deep learning,graph convolution neural networks,graph giltering,spectral approaches},
number = {1},
pages = {97--109},
title = {{CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters}},
volume = {67},
year = {2019}
}
@article{Xie2019,
abstract = {Despite its success, deep learning still needs large labeled datasets to succeed. Data augmentation has shown much promise in alleviating the need for more labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains. In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that it makes use of harder and more realistic noise generated by state-of-the-art data augmentation methods. This small twist leads to substantial improvements on six language tasks and three vision tasks even when the labeled set is extremely small. For example, on the IMDb text classification dataset, with only 20 labeled examples, UDA outperforms the state-of-the-art model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks, CIFAR-10 with 4,000 examples and SVHN with 1,000 examples, UDA outperforms all previous approaches and reduces more than {\$}30\backslashbackslash{\{}\backslash{\%}{\}}{\$} of the error rates of state-of-the-art methods: going from 7.66{\%} to 5.27{\%} and from 3.53{\%} to 2.46{\%} respectively. UDA also works well on datasets that have a lot of labeled data. For example, on ImageNet, with 1.3M extra unlabeled data, UDA improves the top-1/top-5 accuracy from 78.28/94.36{\%} to 79.04/94.45{\%} when compared to AutoAugment.},
archivePrefix = {arXiv},
arxivId = {1904.12848},
author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
eprint = {1904.12848},
pages = {1--15},
title = {{Unsupervised Data Augmentation}},
url = {http://arxiv.org/abs/1904.12848},
year = {2019}
}
@article{Li2019,
abstract = {This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions. First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1904.12787v1},
author = {Li, Yujia and Gu, Chenjie and Dullien, Thomas and Vinyals, Oriol and Kohli, Pushmeet},
eprint = {arXiv:1904.12787v1},
title = {{Graph Matching Networks for Learning the Similarity of Graph Structured Objects}},
url = {https://openreview.net/forum?id=S1xiOjC9F7},
year = {2019}
}
@article{Pang2019,
abstract = {Current state-of-the-art object detection convolutional architectures are manually designed. Although this approach has been successful and delivered strong performances on many benchmarks, their architectures are generally not optimized. For instance, while the backbone models of RetinaNet, and R-CNN family detectors inherit highly optimized architectures from years of research in state-of-art classification networks, their feature pyramid networks which combine features at multiple scales are generally overlooked, and thus under-optimized. Here we aim to learn a better architecture of feature pyramid network for object detection. We search for the best architecture in a form of a directed graph and discover a new feature pyramid network architecture. Our extensive experiments show that the discovered architecture, named NAS-FPN, has many advantages in flexibility and performance. First, NAS-FPN is scalable in that it can be applied repeatedly to improve the performance. Second, not only NAS-FPN works well with the backbone model ResNet-10 that was used during the search process, it also transfers well to other backbone models, such as ResNet-50, ResNet-101 and AmoebaNet. Finally, NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to many state-of-the-art object detection models, including RetinaNet and Mask R-CNN. In particular, our best model achieves 47.5 AP on COCO, which is 2.3 AP better than a single model Mask R-CNN that was pre-trained on much more data. Given the same level of accuracy of 42 AP, our model is 3x faster than Mask R-CNN.},
archivePrefix = {arXiv},
arxivId = {arXiv:1904.07392v1},
author = {Pang, Tsung-yi Lin Ruoming and Le, Quoc V},
eprint = {arXiv:1904.07392v1},
title = {{NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection}},
year = {2019}
}
@article{Feng2019,
abstract = {In this work, we consider one challenging training time attack by modifying training data with bounded perturbation, hoping to manipulate the behavior (both targeted or non-targeted) of any corresponding trained classifier during test time when facing clean samples. To achieve this, we proposed to use an auto-encoder-like network to generate the pertubation on the training data paired with one differentiable system acting as the imaginary victim classifier. The perturbation generator will learn to update its weights by watching the training procedure of the imaginary classifier in order to produce the most harmful and imperceivable noise which in turn will lead the lowest generalization power for the victim classifier. This can be formulated into a non-linear equality constrained optimization problem. Unlike GANs, solving such problem is computationally challenging, we then proposed a simple yet effective procedure to decouple the alternating updates for the two networks for stability. The method proposed in this paper can be easily extended to the label specific setting where the attacker can manipulate the predictions of the victim classifiers according to some predefined rules rather than only making wrong predictions. Experiments on various datasets including CIFAR-10 and a reduced version of ImageNet confirmed the effectiveness of the proposed method and empirical results showed that, such bounded perturbation have good transferability regardless of which classifier the victim is actually using on image data.},
archivePrefix = {arXiv},
arxivId = {1905.09027},
author = {Feng, Ji and Cai, Qi-Zhi and Zhou, Zhi-Hua},
eprint = {1905.09027},
title = {{Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder}},
url = {http://arxiv.org/abs/1905.09027},
year = {2019}
}
@article{Cvpr2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.07853v1},
author = {Cvpr, Anonymous and Id, Paper},
eprint = {arXiv:1905.07853v1},
title = {{Learning Video Representations from Correspondence Proposals}},
year = {2019}
}
@article{Wu2019,
abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories. With a focus on graph convolutional networks, we review alternative architectures that have recently been developed; these learning paradigms include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this fast-growing field.},
archivePrefix = {arXiv},
arxivId = {1901.00596},
author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S},
eprint = {1901.00596},
number = {X},
pages = {1--22},
title = {{A Comprehensive Survey on Graph Neural Networks}},
url = {http://arxiv.org/abs/1901.00596},
volume = {X},
year = {2019}
}
@article{Li2018a,
abstract = {Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). Motivated by online Passive-Agressive (PA) algorithm, we introduce the temporal regularization to SRDCF with single sample, thus resulting in our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Experiments are conducted on three benchmark datasets: OTB-2015, Temple-Color, and VOT-2016. Compared with SRDCF, STRCF with hand-crafted features provides a 5 times speedup and achieves a gain of 5.4{\%} and 3.6{\%} AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF combined with CNN features also performs favorably against state-of-the-art CNN-based trackers and achieves an AUC score of 68.3{\%} on OTB-2015.},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.08679v1},
author = {Li, Feng and Tian, Cheng and Zuo, Wangmeng and Zhang, Lei and Yang, Ming Hsuan},
doi = {10.1109/CVPR.2018.00515},
eprint = {arXiv:1803.08679v1},
file = {:home/arc/Codes/PythonDLCourse/References/STRCF/Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4904--4913},
title = {{Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking}},
year = {2018}
}
@article{Zoph2018,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4{\%} error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.07012v4},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
doi = {10.1109/CVPR.2018.00907},
eprint = {arXiv:1707.07012v4},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8697--8710},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
year = {2018}
}
@article{Surasak2018,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. 1},
author = {Surasak, Thattapon and Takahiro, Ito and Cheng, Cheng Hsuan and Wang, Chi En and Sheng, Pao You},
doi = {10.1109/ICBIR.2018.8391187},
isbn = {9781538652541},
journal = {Proceedings of 2018 5th International Conference on Business and Industrial Research: Smart Technology for Next Generation of Information, Engineering, Business and Social Science, ICBIR 2018},
keywords = {Histogram of Oriented Gradients,Human Detection},
pages = {172--176},
title = {{Histogram of oriented gradients for human detection in video}},
year = {2018}
}
@article{Dai2018,
abstract = {Many graph analytics problems can be solved via iterative algorithms where the solutions are often characterized by a set of steady-state conditions. Different algorithms respect to different set of fixed point constraints, so instead of using these traditional algorithms, can we learn an algorithm which can obtain the same steady-state solutions automatically from examples, in an effective and scalable way? How to represent the meta learner for such algorithm and how to carry out the learning? In this paper, we propose an embedding representation for iterative algorithms over graphs, and design a learning method which alternates between updating the embeddings and projecting them onto the steady-state constraints. We demonstrate the effectiveness of our framework using a few commonly used graph algorithms, and show that in some cases, the learned algorithm can handle graphs with more than 100,000,000 nodes in a single machine.},
author = {Dai, Hanjun and Kozareva, Zornitsa and Dai, Bo and Smola, Alex and Song, Le},
journal = {Pmlr},
pages = {1106--1114},
title = {{Learning Steady-States of Iterative Algorithms over Graphs}},
url = {http://proceedings.mlr.press/v80/dai2018a.html},
volume = {80},
year = {2018}
}
@phdthesis{Takashima2018,
abstract = {1. The electrical activity of up to eight concurrently active motor units has been recorded from the human deltoid and first dorsal interosseous (f.d.i.) muscles. The detected myoelectric signals have been decomposed into their constituent motor-unit action potential trains using a recently developed technique. 2. Concurrently active motor unit behaviour has been examined during triangular force-varying isometric contractions reaching 40 and 80{\%} of maximal voluntary contraction (m.v.c.). Experiments were performed on four normal subjects and three groups of highly trained performers (long-distance swimmers, powerlifters and pianists). 3. Results revealed a highly ordered recruitment and decruitment scheme, based on motoneurone excitability, in both muscles and in all subject groups. 4. Differences were observed between the initial (recruitment) and final (decruitment) firing rates in each muscle. These parameters were invariant with respect to the force rates studied, although some differences were observed among subject groups. 5. In general, firing rates of f.d.i. motor units increased steadily with increasing force (up to 80{\%} m.v.c.). The firing rates of deltoid motor units rose sharply just after recruitment and then increased only slightly thereafter. 6. Recruitment was found to be the major mechanism for generating extra force between 40 and 80{\%} m.v.c. in the deltoid, while rate coding played the major role in the f.d.i. 7. The potential of rate coding for increasing force levels up to m.v.c. is discussed.},
author = {Takashima, Fumiki and Mizunoya, Kazuyuki and Morimoto, Yuji},
booktitle = {Japanese Journal of Anesthesiology},
issn = {00214892},
keywords = {Cross-lock,Non-closed arterial line system,T-shape stopcock},
number = {2},
pages = {203--207},
title = {{Robust Real-time Object Detection}},
volume = {67},
year = {2018}
}
@article{Anumanchipalli2018,
abstract = {The ability to read out, or decode, mental content from brain activity has significant practical and scientific implications. For example, technology that translates cortical activity into speech would be transformative for people unable to communicate as a result of neurological impairment. Decoding speech from neural activity is challenging because speaking requires extremely precise and dynamic control of multiple vocal tract articulators on the order of milliseconds. Here, we designed a neural decoder that explicitly leverages the continuous kinematic and sound representations encoded in cortical activity to generate fluent and intelligible speech. A recurrent neural network first decoded vocal tract physiological signals from direct cortical recordings, and then transformed them to acoustic speech output. Robust decoding performance was achieved with as little as 25 minutes of training data. Naive listeners were able to accurately identify these decoded sentences. Additionally, speech decoding was not only effective for audibly produced speech, but also when participants silently mimed speech. These results advance the development of speech neuroprosthetic technology to restore spoken communication in patients with disabling neurological disorders.},
author = {Anumanchipalli, Gopala K and Chartier, Josh and Chang, Edward F},
doi = {10.1101/481267},
issn = {1476-4687},
journal = {bioRxiv},
pages = {481267},
publisher = {Springer US},
title = {{Intelligible speech synthesis from neural decoding of spoken sentences}},
url = {https://www.biorxiv.org/content/early/2018/11/29/481267},
year = {2018}
}
@article{Redmon2018,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
archivePrefix = {arXiv},
arxivId = {1804.02767},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1804.02767},
file = {:home/arc/Codes/PythonDLCourse/References/Yolo/YOLOv3$\backslash$: An Incremental Improvement.pdf:pdf},
title = {{YOLOv3: An Incremental Improvement}},
url = {http://arxiv.org/abs/1804.02767},
year = {2018}
}
@article{Yu2018,
abstract = {In this report we demonstrate that with same parameters and computational budgets, models with wider features before ReLU activation have significantly better performance for single image super-resolution (SISR). The resulted SR residual network has a slim identity mapping pathway with wider ({\$}\backslash{\$}(2{\$}\backslash{\$}times{\$}\backslash{\$}) to {\$}\backslash{\$}(4{\$}\backslash{\$}times{\$}\backslash{\$})) channels before activation in each residual block. To further widen activation ({\$}\backslash{\$}(6{\$}\backslash{\$}times{\$}\backslash{\$}) to {\$}\backslash{\$}(9{\$}\backslash{\$}times{\$}\backslash{\$})) without computational overhead, we introduce linear low-rank convolution into SR networks and achieve even better accuracy-efficiency tradeoffs. In addition, compared with batch normalization or no normalization, we find training with weight normalization leads to better accuracy for deep super-resolution networks. Our proposed SR network {\$}\backslash{\$}textit{\{}WDSR{\}} achieves better results on large-scale DIV2K image super-resolution benchmark in terms of PSNR with same or lower computational complexity. Based on WDSR, our method also won 1st places in NTIRE 2018 Challenge on Single Image Super-Resolution in all three realistic tracks. Experiments and ablation studies support the importance of wide activation for image super-resolution. Code is released at: https://github.com/JiahuiYu/wdsr{\_}ntire2018},
archivePrefix = {arXiv},
arxivId = {1808.08718},
author = {Yu, Jiahui and Fan, Yuchen and Yang, Jianchao and Xu, Ning and Wang, Zhaowen and Wang, Xinchao and Huang, Thomas},
eprint = {1808.08718},
file = {:home/arc/Codes/PythonDLCourse/References/SRCNN/Wide Activation for Efficient and Accurate Image Super-Resolution.pdf:pdf},
title = {{Wide Activation for Efficient and Accurate Image Super-Resolution}},
url = {http://arxiv.org/abs/1808.08718},
year = {2018}
}
@article{Frankle2018,
abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90{\%}, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20{\%} of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
archivePrefix = {arXiv},
arxivId = {1803.03635},
author = {Frankle, Jonathan and Carbin, Michael},
eprint = {1803.03635},
pages = {1--42},
title = {{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}},
url = {http://arxiv.org/abs/1803.03635},
year = {2018}
}
@article{Shen2018,
abstract = {In general, natural language is governed by a tree structure: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). This is a strict hierarchy: when a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM allows different neurons to track information at different time scales, the architecture does not impose a strict hierarchy. This paper proposes to add such a constraint to the system by ordering the neurons; a vector of "master" input and forget gates ensure that when a given unit is updated, all of the units that follow it in the ordering are also updated. To this end, we propose a new RNN unit: ON-LSTM, which achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.},
archivePrefix = {arXiv},
arxivId = {1810.09536},
author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron},
eprint = {1810.09536},
pages = {1--14},
title = {{Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1810.09536},
year = {2018}
}
@article{Zhang2018,
abstract = {We propose a new network architecture, Gated Attention Networks (GaAN), for learning on graphs. Unlike the traditional multi-head attention mechanism, which equally consumes all attention heads, GaAN uses a convolutional sub-network to control each attention head's importance. We demonstrate the effectiveness of GaAN on the inductive node classification problem. Moreover, with GaAN as a building block, we construct the Graph Gated Recurrent Unit (GGRU) to address the traffic speed forecasting problem. Extensive experiments on three real-world datasets show that our GaAN framework achieves state-of-the-art results on both tasks.},
archivePrefix = {arXiv},
arxivId = {1803.07294},
author = {Zhang, Jiani and Shi, Xingjian and Xie, Junyuan and Ma, Hao and King, Irwin and Yeung, Dit-Yan},
eprint = {1803.07294},
title = {{GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs}},
url = {http://arxiv.org/abs/1803.07294},
year = {2018}
}
@article{Li2018,
abstract = {Graph Convolutional Neural Networks (Graph CNNs) are generalizations of classical CNNs to handle graph data such as molecular data, point could and social networks. Current filters in graph CNNs are built for fixed and shared graph structure. However, for most real data, the graph structures varies in both size and connectivity. The paper proposes a generalized and flexible graph CNN taking data of arbitrary graph structure as input. In that way a task-driven adaptive graph is learned for each graph data while training. To efficiently learn the graph, a distance metric learning is proposed. Extensive experiments on nine graph-structured datasets have demonstrated the superior performance improvement on both convergence speed and predictive accuracy.},
archivePrefix = {arXiv},
arxivId = {1801.03226},
author = {Li, Ruoyu and Wang, Sheng and Zhu, Feiyun and Huang, Junzhou},
eprint = {1801.03226},
title = {{Adaptive Graph Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1801.03226},
year = {2018}
}
@article{Gao2018,
abstract = {Convolutional neural networks (CNNs) have achieved great success on grid-like data such as images, but face tremendous challenges in learning from more generic data such as graphs. In CNNs, the trainable local filters enable the automatic extraction of high-level features. The computation with filters requires a fixed number of ordered units in the receptive fields. However, the number of neighboring units is neither fixed nor are they ordered in generic graphs, thereby hindering the applications of convolutional operations. Here, we address these challenges by proposing the learnable graph convolutional layer (LGCL). LGCL automatically selects a fixed number of neighboring nodes for each feature based on value ranking in order to transform graph data into grid-like structures in 1-D format, thereby enabling the use of regular convolutional operations on generic graphs. To enable model training on large-scale graphs, we propose a sub-graph training method to reduce the excessive memory and computational resource requirements suffered by prior methods on graph convolutions. Our experimental results on node classification tasks in both transductive and inductive learning settings demonstrate that our methods can achieve consistently better performance on the Cora, Citeseer, Pubmed citation network, and protein-protein interaction network datasets. Our results also indicate that the proposed methods using sub-graph training strategy are more efficient as compared to prior approaches.},
archivePrefix = {arXiv},
arxivId = {1808.03965},
author = {Gao, Hongyang and Wang, Zhengyang and Ji, Shuiwang},
doi = {10.1145/3219819.3219947},
eprint = {1808.03965},
isbn = {9781450355520},
keywords = {deep learning,graph convolutional networks,graph mining,large-},
title = {{Large-Scale Learnable Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1808.03965{\%}0Ahttp://dx.doi.org/10.1145/3219819.3219947},
year = {2018}
}
@article{Battaglia2018,
abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
archivePrefix = {arXiv},
arxivId = {1806.01261},
author = {Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
eprint = {1806.01261},
pages = {1--40},
title = {{Relational inductive biases, deep learning, and graph networks}},
url = {http://arxiv.org/abs/1806.01261},
year = {2018}
}
@article{Moravcik2017,
abstract = {Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.01724v1},
author = {Moravcik, Matej and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
doi = {10.1126/science.aam6960.1},
eprint = {arXiv:1701.01724v1},
journal = {Science},
number = {May},
pages = {1--32},
title = {{DeepStack : Expert-Level Artificial Intelligence in No-Limit Poker}},
url = {http://science.sciencemag.org/},
volume = {513},
year = {2017}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.06870v3},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
doi = {10.1109/ICCV.2017.322},
eprint = {arXiv:1703.06870v3},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2980--2988},
title = {{Mask R-CNN}},
volume = {2017-Octob},
year = {2017}
}
@article{Lin2017a,
abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.02002v2},
author = {Lin, Tsung Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
doi = {10.1109/ICCV.2017.324},
eprint = {arXiv:1708.02002v2},
file = {:home/arc/Codes/PythonDLCourse/References/RetinaNet/Focal Loss for Dense Object Detection.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2999--3007},
title = {{Focal Loss for Dense Object Detection}},
volume = {2017-Octob},
year = {2017}
}
@article{Monti2017,
abstract = {Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.08402v3},
author = {Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodol{\`{a}}, Emanuele and Svoboda, Jan and Bronstein, Michael M},
doi = {10.1109/CVPR.2017.576},
eprint = {arXiv:1611.08402v3},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5425--5434},
title = {{Geometric deep learning on graphs and manifolds using mixture model CNNs}},
volume = {2017-Janua},
year = {2017}
}
@article{Lin2017,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.03144v2},
author = {Lin, Tsung Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
doi = {10.1109/CVPR.2017.106},
eprint = {arXiv:1612.03144v2},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {936--944},
title = {{Feature pyramid networks for object detection}},
volume = {2017-Janua},
year = {2017}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v3},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {arXiv:1506.01497v3},
file = {:home/arc/Codes/PythonDLCourse/References/R-CNN/Faster R-CNN$\backslash$: Towards Real-Time Object
Detection with Region Proposal Networks.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27295650},
volume = {39},
year = {2017}
}
@article{Keskar2017,
abstract = {Despite superior training outcomes, adaptive optimization methods such as Adam, Adagrad or RMSprop have been found to generalize poorly compared to Stochastic gradient descent (SGD). These methods tend to perform well in the initial portion of training but are outperformed by SGD at later stages of training. We investigate a hybrid strategy that begins training with an adaptive method and switches to SGD when appropriate. Concretely, we propose SWATS, a simple strategy which switches from Adam to SGD when a triggering condition is satisfied. The condition we propose relates to the projection of Adam steps on the gradient subspace. By design, the monitoring process for this condition adds very little overhead and does not increase the number of hyperparameters in the optimizer. We report experiments on several standard benchmarks such as: ResNet, SENet, DenseNet and PyramidNet for the CIFAR-10 and CIFAR-100 data sets, ResNet on the tiny-ImageNet data set and language modeling with recurrent networks on the PTB and WT2 data sets. The results show that our strategy is capable of closing the generalization gap between SGD and Adam on a majority of the tasks.},
archivePrefix = {arXiv},
arxivId = {1712.07628},
author = {Keskar, Nitish Shirish and Socher, Richard},
eprint = {1712.07628},
number = {1},
title = {{Improving Generalization Performance by Switching from Adam to SGD}},
url = {http://arxiv.org/abs/1712.07628},
year = {2017}
}
@article{Purkait2017,
abstract = {Image based localization is one of the important problems in computer vision due to its wide applicability in robotics, augmented reality, and autonomous systems. There is a rich set of methods described in the literature how to geometrically register a 2D image w.r.t.{\$}\backslashbackslash{\{}\backslash{\$}{\}}a 3D model. Recently, methods based on deep (and convolutional) feedforward networks (CNNs) became popular for pose regression. However, these CNN-based methods are still less accurate than geometry based methods despite being fast and memory efficient. In this work we design a deep neural network architecture based on sparse feature descriptors to estimate the absolute pose of an image. Our choice of using sparse feature descriptors has two major advantages: first, our network is significantly smaller than the CNNs proposed in the literature for this task---thereby making our approach more efficient and scalable. Second---and more importantly---, usage of sparse features allows to augment the training data with synthetic viewpoints, which leads to substantial improvements in the generalization performance to unseen poses. Thus, our proposed method aims to combine the best of the two worlds---feature-based localization and CNN-based pose regression--to achieve state-of-the-art performance in the absolute pose estimation. A detailed analysis of the proposed architecture and a rigorous evaluation on the existing datasets are provided to support our method.},
archivePrefix = {arXiv},
arxivId = {1712.03452},
author = {Purkait, Pulak and Zhao, Cheng and Zach, Christopher},
eprint = {1712.03452},
file = {:home/arc/Codes/PythonDLCourse/References/SPP/SPP-Net$\backslash$: Deep Absolute Pose Regression with Synthetic Views.pdf:pdf},
title = {{SPP-Net: Deep Absolute Pose Regression with Synthetic Views}},
url = {http://arxiv.org/abs/1712.03452},
year = {2017}
}
@article{Chen2017,
abstract = {It is a usual practice to ignore any structural information underlying classes in multi-class classification. In this paper, we propose a graph convolutional network (GCN) augmented neural network classifier to exploit a known, underlying graph structure of labels. The proposed approach resembles an (approximate) inference procedure in, for instance, a conditional random field (CRF). We evaluate the proposed approach on document classification and object recognition and report both accuracies and graph-theoretic metrics that correspond to the consistency of the model's prediction. The experiment results reveal that the proposed model outperforms a baseline method which ignores the graph structures of a label space in terms of graph-theoretic metrics.},
archivePrefix = {arXiv},
arxivId = {1710.04908},
author = {Chen, Meihao and Lin, Zhuoru and Cho, Kyunghyun},
eprint = {1710.04908},
title = {{Graph Convolutional Networks for Classification with a Structured Label Space}},
url = {http://arxiv.org/abs/1710.04908},
year = {2017}
}
@article{Velickovic2017,
abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
archivePrefix = {arXiv},
arxivId = {1710.10903},
author = {Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
eprint = {1710.10903},
pages = {1--12},
title = {{Graph Attention Networks}},
url = {http://arxiv.org/abs/1710.10903},
year = {2017}
}
@article{Hamilton2017,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216},
author = {Hamilton, William L and Ying, Rex and Leskovec, Jure},
eprint = {1706.02216},
number = {Nips},
pages = {1--19},
title = {{Inductive Representation Learning on Large Graphs}},
url = {http://arxiv.org/abs/1706.02216},
year = {2017}
}
@article{Molchanov2017,
abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
archivePrefix = {arXiv},
arxivId = {1701.05369},
author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
eprint = {1701.05369},
pages = {249--256},
title = {{Variational Dropout Sparsifies Deep Neural Networks}},
url = {http://arxiv.org/abs/1701.05369},
volume = {9},
year = {2017}
}
@article{Nickel2017,
abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar{\$}\backslash{\$}'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar{\$}\backslash{\$}'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
archivePrefix = {arXiv},
arxivId = {1705.08039},
author = {Nickel, Maximilian and Kiela, Douwe},
eprint = {1705.08039},
title = {{Poincar{\$}\backslash{\$}'e Embeddings for Learning Hierarchical Representations}},
url = {http://arxiv.org/abs/1705.08039},
year = {2017}
}
@article{Gehring2017,
abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
archivePrefix = {arXiv},
arxivId = {1705.03122},
author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
eprint = {1705.03122},
title = {{Convolutional Sequence to Sequence Learning}},
url = {http://arxiv.org/abs/1705.03122},
year = {2017}
}
@article{Bnjarn2017,
author = {Bnjarn, S Y},
doi = {10.1515/pralin-2017-0007.PBML},
number = {108},
pages = {37--48},
title = {{SY BNjaR0n,jCRN}},
year = {2017}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
issn = {1476-4687},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26819042},
volume = {529},
year = {2016}
}
@article{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashbackslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashbackslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.02325v5},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {arXiv:1512.02325v5},
file = {:home/arc/Codes/PythonDLCourse/References/SSD/SSD$\backslash$: Single Shot MultiBox Detector.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
pages = {21--37},
title = {{SSD: Single shot multibox detector}},
volume = {9905 LNCS},
year = {2016}
}
@article{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.05027v3},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {arXiv:1603.05027v3},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
volume = {9908 LNCS},
year = {2016}
}
@article{Dong2016,
abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.00092v3},
author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
doi = {10.1109/TPAMI.2015.2439281},
eprint = {arXiv:1501.00092v3},
file = {:home/arc/Codes/PythonDLCourse/References/SRCNN/Image Super-Resolution Using Deep
Convolutional Networks.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {2},
pages = {295--307},
pmid = {26761735},
title = {{Image Super-Resolution Using Deep Convolutional Networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26761735},
volume = {38},
year = {2016}
}
@article{Zhang2016,
abstract = {Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance.},
author = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
doi = {10.1109/LSP.2016.2603342},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Cascaded convolutional neural network (CNN),face alignment,face detection},
number = {10},
pages = {1499--1503},
title = {{Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks}},
volume = {23},
year = {2016}
}
@article{AIResearch.com2016,
abstract = {Speech Network Image},
author = {{AI Research (.com)}},
journal = {Http://Airesearch.Com},
pages = {1--27},
title = {{Deep Neural Networks for Acoustic Modeling in Speech Recognition – AI Research}},
url = {http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speech-recognition/},
year = {2016}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
eprint = {1602.07261},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{Defferrard2016,
abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
archivePrefix = {arXiv},
arxivId = {1606.09375},
author = {Defferrard, Micha{\"{e}}l and Bresson, Xavier and Vandergheynst, Pierre},
eprint = {1606.09375},
number = {Nips},
title = {{Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}},
url = {http://arxiv.org/abs/1606.09375},
year = {2016}
}
@article{Kipf2016,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N and Welling, Max},
eprint = {1609.02907},
pages = {1--14},
title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1609.02907},
year = {2016}
}
@article{Niepert2016,
abstract = {Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.},
archivePrefix = {arXiv},
arxivId = {1605.05273},
author = {Niepert, Mathias and Ahmed, Mohamed and Kutzkov, Konstantin},
eprint = {1605.05273},
title = {{Learning Convolutional Neural Networks for Graphs}},
url = {http://arxiv.org/abs/1605.05273},
volume = {1},
year = {2016}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
pages = {1--14},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@article{Bojanowski2016,
abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character {\$}n{\$}-grams. A vector representation is associated to each character {\$}n{\$}-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
archivePrefix = {arXiv},
arxivId = {1607.04606},
author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
eprint = {1607.04606},
title = {{Enriching Word Vectors with Subword Information}},
url = {http://arxiv.org/abs/1607.04606},
year = {2016}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Translation/Google's Neural Machine Translation System$\backslash$: Bridging the Gap between Human and Machine Translation.pdf:pdf},
pages = {1--23},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{Xiong2016,
abstract = {Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the {\$}\backslash{\$}babi-10k text question-answering dataset without supporting fact supervision.},
archivePrefix = {arXiv},
arxivId = {1603.01417},
author = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
eprint = {1603.01417},
title = {{Dynamic Memory Networks for Visual and Textual Question Answering}},
url = {http://arxiv.org/abs/1603.01417},
year = {2016}
}
@article{Xiong2016a,
abstract = {Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0{\%} F1 to 75.9{\%}, while a DCN ensemble obtains 80.4{\%} F1.},
archivePrefix = {arXiv},
arxivId = {1611.01604},
author = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
eprint = {1611.01604},
pages = {1--14},
title = {{Dynamic Coattention Networks For Question Answering}},
url = {http://arxiv.org/abs/1611.01604},
year = {2016}
}
@article{Nallapati2016,
abstract = {In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
archivePrefix = {arXiv},
arxivId = {1602.06023},
author = {Nallapati, Ramesh and Zhou, Bowen and dos Santos, Cicero Nogueira and Gulcehre, Caglar and Xiang, Bing},
eprint = {1602.06023},
title = {{Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond}},
url = {http://arxiv.org/abs/1602.06023},
year = {2016}
}
@article{Bradbury2016,
abstract = {Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.},
archivePrefix = {arXiv},
arxivId = {1611.01576},
author = {Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
eprint = {1611.01576},
pages = {1--11},
title = {{Quasi-Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1611.01576},
year = {2016}
}
@article{Perez2016,
abstract = {Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.},
archivePrefix = {arXiv},
arxivId = {1610.04211},
author = {Perez, Julien and Liu, Fei},
eprint = {1610.04211},
pages = {1--11},
title = {{Gated End-to-End Memory Networks}},
url = {http://arxiv.org/abs/1610.04211},
year = {2016}
}
@article{Inan2016,
abstract = {Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.},
archivePrefix = {arXiv},
arxivId = {1611.01462},
author = {Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
eprint = {1611.01462},
pages = {1--13},
title = {{Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling}},
url = {http://arxiv.org/abs/1611.01462},
year = {2016}
}
@article{Hashimoto2016,
abstract = {Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.},
archivePrefix = {arXiv},
arxivId = {1611.01587},
author = {Hashimoto, Kazuma and Xiong, Caiming and Tsuruoka, Yoshimasa and Socher, Richard},
eprint = {1611.01587},
title = {{A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks}},
url = {http://arxiv.org/abs/1611.01587},
year = {2016}
}
@article{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V},
eprint = {1611.01578},
pages = {1--16},
title = {{Neural Architecture Search with Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.01578},
year = {2016}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08083v2},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {arXiv:1504.08083v2},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1440--1448},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{Kuo2015,
abstract = {Existing object proposal approaches use primarily bottom-up cues to rank proposals, while we believe that objectness is in fact a high level construct. We argue for a data-driven, semantic approach for ranking object proposals. Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method. We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster. We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP. Our implementation achieves this performance while running at 260 ms per image.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.02146v2},
author = {Kuo, Weicheng and Hariharan, Bharath and Malik, Jitendra},
doi = {10.1109/ICCV.2015.285},
eprint = {arXiv:1505.02146v2},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2479--2487},
title = {{DeepBox: Learning objectness with convolutional networks}},
volume = {2015 Inter},
year = {2015}
}
@article{Yang2015,
abstract = {In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99{\%} on the challenging FDDB benchmark, outperforming the state-of-the-art method by a large margin of 2.91{\%}. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06451v1},
author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
doi = {10.1109/ICCV.2015.419},
eprint = {arXiv:1509.06451v1},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {3},
pages = {3676--3684},
title = {{From facial parts responses to face detection: A deep learning approach}},
volume = {2015 Inter},
year = {2015}
}
@article{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.4842v1},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {arXiv:1409.4842v1},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {1476-4687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25719670},
volume = {518},
year = {2015}
}
@article{He2015a,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224 × 224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 × faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank {\#}2 in object detection and {\#}3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.4729v4},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {arXiv:1406.4729v4},
file = {:home/arc/Codes/PythonDLCourse/References/SPP/Spatial pyramid pooling
in deep convolutional networks for visual recognition.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Neural Networks,Image Classification,Object Detection,Spatial Pyramid Pooling},
number = {9},
pages = {1904--1916},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
volume = {37},
year = {2015}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {1511.06434},
pages = {1--16},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Shi2015,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
archivePrefix = {arXiv},
arxivId = {1506.04214},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
eprint = {1506.04214},
pages = {1--12},
title = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
url = {http://arxiv.org/abs/1506.04214},
year = {2015}
}
@article{Tang2015,
abstract = {This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the "LINE," which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.},
archivePrefix = {arXiv},
arxivId = {1503.03578},
author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
doi = {10.1145/2736277.2741093},
eprint = {1503.03578},
isbn = {9781450334693},
keywords = {dimension reduction,feature learn-,information network embedding,ing,scalability},
title = {{LINE: Large-scale Information Network Embedding}},
url = {http://arxiv.org/abs/1503.03578{\%}0Ahttp://dx.doi.org/10.1145/2736277.2741093},
year = {2015}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {:home/arc/Codes/PythonDLCourse/References/Yolo/You Only Look Once$\backslash$:
Unified, Real-Time Object Detection.pdf:pdf},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Lipton2015,
abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
archivePrefix = {arXiv},
arxivId = {1506.00019},
author = {Lipton, Zachary C and Berkowitz, John and Elkan, Charles},
eprint = {1506.00019},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning}},
url = {http://arxiv.org/abs/1506.00019},
year = {2015}
}
@article{Henaff2015,
abstract = {Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.},
archivePrefix = {arXiv},
arxivId = {1506.05163},
author = {Henaff, Mikael and Bruna, Joan and LeCun, Yann},
eprint = {1506.05163},
pages = {1--10},
title = {{Deep Convolutional Networks on Graph-Structured Data}},
url = {http://arxiv.org/abs/1506.05163},
year = {2015}
}
@article{Szegedy2015a,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
eprint = {1512.00567},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Kiros2015,
abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.06726},
author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
eprint = {1506.06726},
number = {786},
pages = {1--11},
title = {{Skip-Thought Vectors}},
url = {http://arxiv.org/abs/1506.06726},
year = {2015}
}
@article{Bartunov2015,
abstract = {Recently proposed Skip-gram model is a powerful method for learning high-dimensional word representations that capture rich semantic relationships between words. However, Skip-gram as well as most prior work on learning word representations does not take into account word ambiguity and maintain only single representation per word. Although a number of Skip-gram modifications were proposed to overcome this limitation and learn multi-prototype word representations, they either require a known number of word meanings or learn them using greedy heuristic approaches. In this paper we propose the Adaptive Skip-gram model which is a nonparametric Bayesian extension of Skip-gram capable to automatically learn the required number of representations for all words at desired semantic resolution. We derive efficient online variational learning algorithm for the model and empirically demonstrate its efficiency on word-sense induction task.},
archivePrefix = {arXiv},
arxivId = {1502.07257},
author = {Bartunov, Sergey and Kondrashkin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
eprint = {1502.07257},
pages = {1--15},
title = {{Breaking Sticks and Ambiguities with Adaptive Skip-gram}},
url = {http://arxiv.org/abs/1502.07257},
year = {2015}
}
@article{Trask2015,
abstract = {Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or "senses". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8{\%} average error reduction in unlabeled attachment scores across 6 languages.},
archivePrefix = {arXiv},
arxivId = {1511.06388},
author = {Trask, Andrew and Michalak, Phil and Liu, John},
eprint = {1511.06388},
pages = {1--9},
title = {{sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings}},
url = {http://arxiv.org/abs/1511.06388},
year = {2015}
}
@article{Shang2015,
abstract = {We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75{\%} of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.},
archivePrefix = {arXiv},
arxivId = {1503.02364},
author = {Shang, Lifeng and Lu, Zhengdong and Li, Hang},
eprint = {1503.02364},
title = {{Neural Responding Machine for Short-Text Conversation}},
url = {http://arxiv.org/abs/1503.02364},
year = {2015}
}
@article{Vinyals2015,
abstract = {Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.},
archivePrefix = {arXiv},
arxivId = {1506.05869},
author = {Vinyals, Oriol and Le, Quoc},
eprint = {1506.05869},
keywords = { chatbots, dialog systems,neural networks},
title = {{A Neural Conversational Model}},
url = {http://arxiv.org/abs/1506.05869},
volume = {37},
year = {2015}
}
@article{Sordoni2015,
abstract = {We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.},
archivePrefix = {arXiv},
arxivId = {1506.06714},
author = {Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
eprint = {1506.06714},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Dialogue and Conversations/A Neural Network Approach to
Context-Sensitive Generation of Conversational Responses.pdf:pdf},
title = {{A Neural Network Approach to Context-Sensitive Generation of Conversational Responses}},
url = {http://arxiv.org/abs/1506.06714},
year = {2015}
}
@article{Kumar2015,
abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
archivePrefix = {arXiv},
arxivId = {1506.07285},
author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
eprint = {1506.07285},
title = {{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing}},
url = {http://arxiv.org/abs/1506.07285},
year = {2015}
}
@article{Rush2015,
abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
archivePrefix = {arXiv},
arxivId = {1509.00685},
author = {Rush, Alexander M and Chopra, Sumit and Weston, Jason},
eprint = {1509.00685},
title = {{A Neural Attention Model for Abstractive Sentence Summarization}},
url = {http://arxiv.org/abs/1509.00685},
year = {2015}
}
@article{Hermann2015,
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
archivePrefix = {arXiv},
arxivId = {1506.03340},
author = {Hermann, Karl Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
eprint = {1506.03340},
pages = {1--14},
title = {{Teaching Machines to Read and Comprehend}},
url = {http://arxiv.org/abs/1506.03340},
year = {2015}
}
@article{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
eprint = {1502.03044},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@article{Luong2015,
abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
archivePrefix = {arXiv},
arxivId = {1508.04025},
author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
eprint = {1508.04025},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Attention/Effective Approaches to Attention-based Neural Machine Translation.pdf:pdf},
title = {{Effective Approaches to Attention-based Neural Machine Translation}},
url = {http://arxiv.org/abs/1508.04025},
year = {2015}
}
@article{Tai2015,
abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
archivePrefix = {arXiv},
arxivId = {1503.00075},
author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
eprint = {1503.00075},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Improved Semantic Representations From
Tree-Structured Long Short-Term Memory Networks.pdf:pdf},
title = {{Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks}},
url = {http://arxiv.org/abs/1503.00075},
year = {2015}
}
@article{Huang2015,
abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
archivePrefix = {arXiv},
arxivId = {1508.01991},
author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
eprint = {1508.01991},
title = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
url = {http://arxiv.org/abs/1508.01991},
year = {2015}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Hadidi2014,
abstract = {Deep Neural Networks (DNNs) have recently shown outstanding performance on the task of whole image classification. In this paper we go one step further and address the problem of object detection -- not only classifying but also precisely localizing objects of various classes using DNNs. We present a simple and yet powerful formulation of object detection as a regression to object masks. We define a multi-scale inference procedure which is able to produce a high-resolution object detection at a low cost by a few network applications. The approach achieves state-of-the-art performance on Pascal 2007 VOC.},
author = {Hadidi, Niloufar Niakosari and Cullen, Kathryn R and Hall, Leah M J and Lindquist, Ruth and Buckwalter, Kathleen C and Mathews, Emily},
doi = {10.3928/19404921-20140820-01},
issn = {1940-4921},
journal = {Research in Gerontological Nursing},
number = {5},
pages = {200--205},
title = {{Functional Magnetic Resonance Imaging as Experienced by Stroke Survivors}},
volume = {7},
year = {2014}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2524v5},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {arXiv:1311.2524v5},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@article{Zitnick2014,
abstract = {The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box's boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96{\%} object recall at overlap threshold of 0.5 and over 75{\%} recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
author = {Zitnick, C Lawrence and Doll{\'{a}}r, Piotr},
doi = {10.1007/978-3-319-10602-1_26},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {edge detection,object detection,object proposals},
number = {PART 5},
pages = {391--405},
title = {{Edge boxes: Locating object proposals from edges}},
volume = {8693 LNCS},
year = {2014}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\$}\backslash{\$}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1311.2901v3},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@article{Denton2014,
abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1{\%} of the original model.},
archivePrefix = {arXiv},
arxivId = {1404.0736},
author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
eprint = {1404.0736},
file = {:home/arc/Codes/PythonDLCourse/References/R-CNN/Exploiting Linear Structure Within Convolutional
Networks for Efficient Evaluation.pdf:pdf},
pages = {1--11},
title = {{Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation}},
url = {http://arxiv.org/abs/1404.0736},
year = {2014}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {1406.2661},
pages = {1--9},
title = {{Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
pages = {1--14},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
eprint = {1409.2329},
number = {2013},
pages = {1--8},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
eprint = {1409.3215},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Translation/Sequence to Sequence Learning with Neural Networks.pdf:pdf},
pages = {1--9},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@article{Bahdanau2014,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473},
pages = {1--15},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473},
year = {2014}
}
@article{Sutskever2014a,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
eprint = {1409.3215},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Translation/Sequence to Sequence Learning with Neural Networks.pdf:pdf},
pages = {1--9},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
eprint = {1412.3555},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
url = {http://arxiv.org/abs/1412.3555},
year = {2014}
}
@article{Vinyals2014,
abstract = {Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.},
archivePrefix = {arXiv},
arxivId = {1412.7449},
author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
eprint = {1412.7449},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Attention/Grammar as a Foreign Language.pdf:pdf},
pages = {1--10},
title = {{Grammar as a Foreign Language}},
url = {http://arxiv.org/abs/1412.7449},
year = {2014}
}
@article{Sande2013,
abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce Selective Search which combines the strength of both an exhaustive search and seg-mentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our Selective Search results in a small set of data-driven, class-independent, high quality locations, yielding 99{\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The Selective Search software is made publicly available 1 .},
author = {Sande, Koen Van De},
journal = {International Journal of Computer Vision},
keywords = {()},
title = {{Selective Search for Object Localisation}},
url = {http://disi.unitn.it/},
year = {2013}
}
@article{RichardSocherAlexPerelyginJeanY.WuJasonChuangChristopherD.Manning2013,
abstract = {{\textcopyright}2013 Association for Computational Linguistics. Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
archivePrefix = {arXiv},
arxivId = {1690219.1690245‎},
author = {{Richard Socher, Alex Perelygin, Jean Y.Wu, Jason Chuang, Christopher D. Manning}, Andrew Y Ng and Potts, Christopher},
doi = {10.1371/journal.pone.0073791},
eprint = {1690219.1690245‎},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {1631--1642},
pmid = {24086296},
title = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
url = {http://nlp.stanford.edu/{\%}7B{~}{\%}7Dsocherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf},
volume = {8},
year = {2013}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{Erhan2013,
abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
archivePrefix = {arXiv},
arxivId = {1312.2249},
author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
eprint = {1312.2249},
title = {{Scalable Object Detection using Deep Neural Networks}},
url = {http://arxiv.org/abs/1312.2249},
year = {2013}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
pages = {1--10},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Bruna2013,
abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
archivePrefix = {arXiv},
arxivId = {1312.6203},
author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
eprint = {1312.6203},
pages = {1--14},
title = {{Spectral Networks and Locally Connected Networks on Graphs}},
url = {http://arxiv.org/abs/1312.6203},
year = {2013}
}
@article{Socher2012,
author = {Socher, Richard and Huval, Brody and Manning, C D and Ng, A Y},
journal = {Proceedings of the 2012 Joint {\ldots}},
number = {Mv},
title = {基于递归矩阵-向量空间的语义组合},
url = {http://dl.acm.org/citation.cfm?id=2391084},
year = {2012}
}
@article{Krauss2012,
abstract = {CXCR4 is a G-protein coupled receptor for CXCL12 that plays an important role in human immunodeficiency virus infection, cancer growth and metastasization, immune cell trafficking and WHIM syndrome. In the absence of an X-ray crystal structure, theoretical modeling of the CXCR4 receptor remains an important tool for structure-function analysis and to guide the discovery of new antagonists with potential clinical use. In this study, the combination of experimental data and molecular modeling approaches allowed the development of optimized ligand-receptor models useful for elucidation of the molecular determinants of small molecule binding and functional antagonism. The ligand-guided homology modeling approach used in this study explicitly re-shaped the CXCR4 binding pocket in order to improve discrimination between known CXCR4 antagonists and random decoys. Refinement based on multiple test-sets with small compounds from single chemotypes provided the best early enrichment performance. These results provide an important tool for structure-based drug design and virtual ligand screening of new CXCR4 antagonists.},
author = {Krauss, Ronald M and Nichols, Alex V},
doi = {10.1007/978-1-4684-1262-8_2},
journal = {Lipoprotein Deficiency Syndromes},
pages = {17--27},
title = {{A Convolutional Neural Network Cascade for Face Detection}},
year = {2012}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{2012 AlexNet}},
year = {2012}
}
@article{Arbelaez2011,
abstract = {This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.},
author = {Arbel{\'{a}}ez, Pablo and Maire, Michael and Fowlkes, Charless and Malik, Jitendra},
doi = {10.1109/TPAMI.2010.161},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {5},
pages = {898--916},
pmid = {20733228},
title = {{Contour detection and hierarchical image segmentation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20733228},
volume = {33},
year = {2011}
}
@article{Socher2008,
author = {Socher, Richard and Bauer, John and Manning, Christopher D and Ng, Andrew Y},
title = {{SocherBauerManningNg{\_}ACL2013.pdf}},
year = {2008}
}
@article{VonLuxburg2007,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {{Von Luxburg}, Ulrike},
doi = {10.1007/s11222-007-9033-z},
eprint = {arXiv:0711.0189v1},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Graph Laplacian,Spectral clustering},
number = {4},
pages = {395--416},
title = {{A tutorial on spectral clustering}},
volume = {17},
year = {2007}
}
@article{Dhillon2007,
abstract = {A variety of clustering algorithms have recently been proposed to handle data that is not linearly separable; spectral clustering and kernel k-means are two of the main methods. In this paper, we discuss an equivalence between the objective functions used in these seemingly different methods--in particular, a general weighted kernel k-means objective is mathematically equivalent to a weighted graph clustering objective. We exploit this equivalence to develop a fast, high-quality multilevel algorithm that directly optimizes various weighted graph clustering objectives, such as the popular ratio cut, normalized cut, and ratio association criteria. This eliminates the need for any eigenvector computation for graph clustering problems, which can be prohibitive for very large graphs. Previous multilevel graph partitioning methods, such as Metis, have suffered from the restriction of equal-sized clusters; our multilevel algorithm removes this restriction by using kernel k-means to optimize weighted graph cuts. Experimental results show that our multilevel algorithm outperforms a state-of-the-art spectral clustering algorithm in terms of speed, memory usage, and quality. We demonstrate that our algorithm is applicable to large-scale clustering tasks such as image segmentation, social network analysis and gene network analysis.},
author = {Dhillon, Inderjit S and Guan, Yuqiang and Kulis, Brian},
doi = {10.1109/TPAMI.2007.1115},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Clustering,Data mining,Graph partitioning,K-means,Kernel,Segmentation,Spectral clustering,k-means},
number = {11},
pages = {1944--1957},
title = {{Weighted graph cuts without eigenvectors a multilevel approach}},
volume = {29},
year = {2007}
}
@article{Kushnir2006,
abstract = {We present a novel multiscale clustering algorithm inspired by algebraic multigrid techniques. Our method begins with assembling data points according to local similarities. It uses an aggregation process to obtain reliable scale-dependent global properties, which arise from the local similarities. As the aggregation process proceeds, these global properties affect the formation of coherent clusters. The global features that can be utilized are for example density, shape, intrinsic dimensionality and orientation. The last three features are a part of the manifold identification process which is performed in parallel to the clustering process. The algorithm detects clusters that are distinguished by their multiscale nature, separates between clusters with different densities, and identifies and resolves intersections between clusters. The algorithm is tested on synthetic and real data sets, its running time complexity is linear in the size of the data set. {\textcopyright}2006 Pattern Recognition Society.},
author = {Kushnir, Dan and Galun, Meirav and Brandt, Achi},
doi = {10.1016/j.patcog.2006.04.007},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Aggregation,Algebraic multigrid (AMG),Astrophysical models,Data analysis,Graph partitioning,Manifold,Similarity-based clustering},
number = {10},
pages = {1876--1891},
title = {{Fast multiscale clustering and manifold identification}},
volume = {39},
year = {2006}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of affine distortion , change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G},
journal = {International Journal of Computer Vision},
pages = {1--28},
title = {{Distinctive Image Features from Scale-Invariant Keypoints .pdf}},
year = {2004}
}
@article{,
journal = {proc. OF THE IEEE},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/{\#}full-text-section},
year = {1998}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Karypis1995,
abstract = {partition algorithm},
author = {Karypis, George and Kumar, Vipin},
journal = {Unstructured Graph Partinioning and Sparse Matrix Ordering},
pages = {1--16},
title = {{Metis: Unstructured Graph Partitioning and Sparse Matrix Ordering}},
year = {1995}
}
@misc{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.},
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
booktitle = {Nature},
doi = {10.1038/323533a0},
pages = {533--536},
title = {{Backprop{\_}Old.Pdf}},
volume = {323},
year = {1986}
}
@article{Mackay1875,
abstract = {PURPOSE: We aimed to determine whether there was a difference in post-operative symptomatic control and quality of life (QoL) between patients who were obese (BMI {\textgreater}30) and non-obese (BMI {\textless}30) pre-operatively. This information may inform the decision making of Physicians and patients whether to proceed to surgery for management of symptomatic lumbar disc prolapse. METHODS: We conducted a prospective questionnaire-based study of QoL and symptom control in 120 patients with postal follow-up at 3 and 12 months after lumbar disc surgery. This study was conducted in two United Kingdom regional neurosurgical units, with ethical approval from the North of Scotland Research Ethics Service (09/S0801/7). RESULTS: 120 patients were recruited; 37 (34.5{\%}) were obese. Follow up was 71{\%} at 3 months and 57{\%} at 12 months. At recruitment, both obese and non-obese patient groups had similar functional status and pain scores. At 3 and 12 months, non-obese and obese patients reported similar and significant benefits from surgery (e.g. 12 month SF-36 80.5 vs. 68.8, respectively). In non-obese and obese patients, time to return to work was 47.5 days and 53.8 days, respectively, (p = .345). After 12 months all QoL scores were significantly improved from pre-operative levels in both groups. CONCLUSIONS: Obese patients derive significant benefit from lumbar discectomy that it is similar to the benefit experienced by non-obese patients. Obese individuals may achieve excellent results from discectomy and these patients should not be refused surgery on the basis of BMI alone.},
author = {Mackay, Charles},
doi = {10.1093/nq/s5-IV.96.346-c},
issn = {00293970},
journal = {Notes and Queries},
number = {96},
pages = {346},
title = {{"Glove."}},
volume = {s5-IV},
year = {1875}
}
@article{Berthelot,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.02249v1},
author = {Berthelot, David and Oliver, Avital and Carlini, Nicholas and Goodfellow, Ian and Raffel, Colin and Papernot, Nicolas},
eprint = {arXiv:1905.02249v1},
title = {{MixMatch : A Holistic Approach to Semi-Supervised Learning arXiv : 1905 . 02249v1 [ cs . LG ] 6 May 2019}}
}
@misc{,
title = {{【LeNet-5】.pdf}}
}
@article{Lin,
archivePrefix = {arXiv},
arxivId = {arXiv:1207.6324},
author = {Lin, Cliff Chiung-yu and Ng, Andrew Y and Manning, Christopher D},
doi = {10.1007/978-3-540-87479-9},
eprint = {arXiv:1207.6324},
isbn = {9781450306195},
issn = {{\textless}null{\textgreater}},
pmid = {22183238},
title = {{New environment norms on so}}
}
@article{Dean,
archivePrefix = {arXiv},
arxivId = {fa},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
doi = {10.1109/ICDAR.2011.95},
eprint = {fa},
isbn = {9781627480031},
issn = {10495258},
pages = {1--11},
pmid = {43479959},
title = {{Large Scale Distributed Deep Networks}}
}
@article{Rush2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.00685v2},
author = {Rush, Alexander M and Weston, Jason},
eprint = {arXiv:1509.00685v2},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Abstract/A Neural Attention Model for Abstractive Sentence Summarization.pdf:pdf},
title = {{A Neural Attention Model for Abstractive Sentence Summarization}},
year = {2014}
}
@article{Xiang2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.06023v5},
author = {Xiang, Bing},
eprint = {arXiv:1602.06023v5},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Abstract/Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.pdf:pdf},
title = {{Cicero dos Santos}},
year = {2014}
}
@article{Xu2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03044v3},
author = {Xu, Kelvin and Courville, Aaron and Zemel, Richard S and Bengio, Yoshua},
eprint = {arXiv:1502.03044v3},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Attention/Show, Attend and Tell$\backslash$: Neural Image Caption
Generation with Visual Attention.pdf:pdf},
title = {{Show , Attend and Tell : Neural Image Caption Generation with Visual Attention}},
year = {2014}
}
@article{Moritz,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03340v3},
author = {Moritz, Karl and Tom, Hermann and Kay, Will},
eprint = {arXiv:1506.03340v3},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Attention/Teaching Machines to Read and Comprehend.pdf:pdf},
pages = {1--14},
title = {{Teaching Machines to Read and Comprehend}}
}
@article{Yu2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1508.01991v1},
author = {Yu, Kai},
eprint = {arXiv:1508.01991v1},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Bidirectional LSTM-CRF Models for Sequence Tagging.pdf:pdf},
title = {{Bidirectional LSTM-CRF Models for Sequence Tagging}},
year = {2011}
}
@article{Com2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.05869v3},
author = {Com, Q V L Google},
eprint = {arXiv:1506.05869v3},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Dialogue and Conversations/A Neural Conversational Model.pdf:pdf},
keywords = {neural networks, dialog systems, chatbots},
title = {{A Neural Conversational Model}},
volume = {37},
year = {2015}
}
@article{Shang2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.02364v2},
author = {Shang, Lifeng and Lu, Zhengdong and Li, Hang},
eprint = {arXiv:1503.02364v2},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Dialogue and Conversations/Neural Responding Machine for Short-Text Conversation.pdf:pdf},
title = {{Neural Responding Machine for Short-Text Conversation}},
year = {2011}
}
@article{Lina,
author = {Lin, Cliff Chiung-yu and Ng, Andrew Y and Manning, Christopher D},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Emotions/Parsing Natural Scenes and Natural Language with Recursive Neural Networks.pdf:pdf},
title = {{Parsing Natural Scenes and Natural Language}}
}
@article{Socher2008a,
author = {Socher, Richard and Bauer, John and Manning, Christopher D and Ng, Andrew Y},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Emotions/Parsing with Compositional Vector Grammars.pdf:pdf},
title = {{Parsing with Compositional Vector Grammars}},
year = {2008}
}
@article{Socher2013,
author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Emotions/Recursive Deep Models for Semantic Compositionality
Over a Sentiment Treebank.pdf:pdf},
title = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
year = {2013}
}
@article{Socher2011,
author = {Socher, Richard and Huval, Brody and Manning, Christopher D and Ng, Andrew Y},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Emotions/Semantic Compositionality through Recursive Matrix-Vector Spaces.pdf:pdf},
number = {Mv},
title = {{Semantic Compositionality through Recursive Matrix-Vector Spaces}},
year = {2011}
}
@article{Sukhbaatar,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08895v5},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur},
eprint = {arXiv:1503.08895v5},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/End-To-End Memory Networks.pdf:pdf},
pages = {1--11},
title = {{End-To-End Memory Networks}}
}
@article{Kumar2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.07285v5},
author = {Kumar, Ankit and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
eprint = {arXiv:1506.07285v5},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/QA System/Ask Me Anything$\backslash$:
Dynamic Memory Networks for Natural Language Processing.pdf:pdf},
title = {{Ask Me Anything : Dynamic Memory Networks for Natural Language Processing}},
year = {2016}
}
@article{Zhong2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01604v4},
author = {Zhong, Victor and Socher, Richard},
eprint = {arXiv:1611.01604v4},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/QA System/DYNAMIC COATTENTION NETWORKS FOR QUESTION ANSWERING.pdf:pdf},
pages = {1--14},
title = {{D YNAMIC C OATTENTION N ETWORKS}},
year = {2017}
}
@article{,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.01417v1},
eprint = {arXiv:1603.01417v1},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/QA System/Dynamic Memory Networks for Visual and Textual Question Answering.pdf:pdf},
title = {{Dynamic Memory Networks for Visual and Textual Question Answering}},
year = {2015}
}
@article{Merity2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01576v2},
author = {Merity, Stephen and Xiong, Caiming and Socher, Richard},
eprint = {arXiv:1611.01576v2},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/QUASI-RECURRENT NEURAL NETWORKS.pdf:pdf},
pages = {1--11},
title = {{Q -r n n}},
year = {2017}
}
@article{Gram,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07257v2},
author = {Gram, A Daptive S K I P and Osokin, Anton},
eprint = {arXiv:1502.07257v2},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Text Embeddings/BREAKING STICKS AND AMBIGUITIES WITH ADAPTIVE SKIP-GRAM.pdf:pdf},
pages = {1--15},
title = {{B REAKING S TICKS AND A MBIGUITIES WITH}}
}
@article{Mikolov,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
eprint = {arXiv:1301.3781v3},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Text Embeddings/Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
pages = {1--12},
title = {{Vector Space}}
}
@article{Bojanowski1996,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.04606v2},
author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
eprint = {arXiv:1607.04606v2},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Text Embeddings/Enriching Word Vectors with Subword Information.pdf:pdf},
title = {{Enriching Word Vectors with Subword Information}},
year = {1996}
}
@article{Pennington,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Text Embeddings/GloVe$\backslash$: Global Vectors for Word Representation.pdf:pdf},
title = {{GloVe : Global Vectors for Word Representation}}
}
@article{Nickel,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.08039v2},
author = {Nickel, Maximilian},
eprint = {arXiv:1705.08039v2},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Text Embeddings/Poincar{\'{e}} Embeddings for
Learning Hierarchical Representations.pdf:pdf},
title = {{Poincar{\'{e}} Embeddings for Learning Hierarchical Representations}}
}
@article{Trask2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06388v1},
author = {Trask, Andrew and Michalak, Phil and Liu, John},
eprint = {arXiv:1511.06388v1},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Text Embeddings/SENSE2VEC - A FAST AND ACCURATE METHOD FOR WORD SENSE DISAMBIGUATION IN NEURAL WORD EMBEDDINGS.pdf:pdf},
pages = {1--9},
title = {{FOR WORD SENSE DISAMBIGUATION IN NEURAL WORD EMBEDDINGS .}},
year = {2016}
}
@article{Kiros,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.06726v1},
author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
eprint = {arXiv:1506.06726v1},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Text Embeddings/Skip-Thought Vectors.pdf:pdf},
number = {786},
pages = {1--11},
title = {{Skip-Thought Vectors arXiv : 1506 . 06726v1 [ cs . CL ] 22 Jun 2015}}
}
@article{Bulletin2017,
author = {Bulletin, The Prague and Linguistics, Mathematical},
doi = {10.1515/pralin-2017-0007.PBML},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Translation/Convolutional over Recurrent Encoder for Neural Machine Translation.pdf:pdf},
number = {108},
pages = {37--48},
title = {{The Prague Bulletin of Mathematical Linguistics}},
year = {2017}
}
@article{Gehring2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.03122v3},
author = {Gehring, Jonas and Dauphin, Yann N},
eprint = {arXiv:1705.03122v3},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Translation/Convolutional Sequence to Sequence Learning.pdf:pdf},
title = {{Convolutional Sequence to Sequence Learning}},
year = {2016}
}
@article{Chung,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.3555v1},
author = {Chung, Junyoung},
eprint = {arXiv:1412.3555v1},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Translation/Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:pdf},
pages = {1--9},
title = {{Gated Recurrent Neural Networks on Sequence Modeling arXiv : 1412 . 3555v1 [ cs . NE ] 11 Dec 2014}}
}
@article{Bahdanau2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.0473v6},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {arXiv:1409.0473v6},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/Translation/NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE.pdf:pdf},
pages = {1--15},
title = {{N EURAL M ACHINE T RANSLATION}},
year = {2015}
}
@article{Socher2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01462v3},
author = {Socher, Richard},
eprint = {arXiv:1611.01462v3},
file = {:home/arc/Codes/PythonDLCourse/References/NLP/TYING WORD VECTORS AND WORD CLASSIFIERS $\backslash$:A LOSS FRAMEWORK FOR LANGUAGE MODELING.pdf:pdf},
pages = {1--13},
title = {{T YING W ORD V ECTORS AND W ORD C LASSIFIERS :}},
year = {2017}
}
@article{Etworks2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.09536v5},
author = {Etworks, N Eural N and Universit, Mila and Sordoni, Alessandro},
eprint = {arXiv:1810.09536v5},
file = {:home/arc/Codes/PythonDLCourse/References/ON-LSTM/on-lstm.pdf:pdf},
pages = {1--14},
title = {{O n : i t s n n}},
year = {2019}
}
@article{Girshick,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08083v2},
author = {Girshick, Ross},
eprint = {arXiv:1504.08083v2},
file = {:home/arc/Codes/PythonDLCourse/References/R-CNN/Fast R-CNN.pdf:pdf},
title = {{Fast R-CNN}}
}
@article{R-cnn,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.06870v3},
author = {R-cnn, Mask and Doll, Piotr and Girshick, Ross},
eprint = {arXiv:1703.06870v3},
file = {:home/arc/Codes/PythonDLCourse/References/R-CNN/Mask R-CNN.pdf:pdf},
title = {{Mask R-CNN}}
}
@article{Girshick2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2524v5},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra and Berkeley, U C},
eprint = {arXiv:1311.2524v5},
file = {:home/arc/Codes/PythonDLCourse/References/R-CNN/Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2012}
}
@article{Lipton2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.00019v1},
author = {Lipton, Zachary C},
eprint = {arXiv:1506.00019v1},
file = {:home/arc/Codes/PythonDLCourse/References/RNN/A Critical Review of Recurrent Neural Networks
for Sequence Learning.pdf:pdf},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning arXiv : 1506 . 00019v1 [ cs . LG ] 29 May 2015}},
year = {2015}
}
@article{Hinton,
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
file = {:home/arc/Codes/PythonDLCourse/References/RNN/Deep Neural Networks for Acoustic Modeling
in Speech Recognition.pdf:pdf},
pages = {1--27},
title = {{Deep Neural Networks for Acoustic Modeling in Speech Recognition}}
}
@article{Hochreiter2016,
author = {Hochreiter, Sepp},
doi = {10.1162/neco.1997.9.8.1735},
file = {:home/arc/Codes/PythonDLCourse/References/RNN/lstm.pdf:pdf},
number = {December 1997},
title = {{Long Short-term Memory}},
year = {2016}
}
@article{Zaremba2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.2329v5},
author = {Zaremba, Wojciech},
eprint = {arXiv:1409.2329v5},
file = {:home/arc/Codes/PythonDLCourse/References/RNN/Recurrent Neural Network Regularization.pdf:pdf},
number = {2013},
pages = {1--8},
title = {{R n n r}},
year = {2015}
}
@article{Sutskever,
author = {Sutskever, Ilya},
file = {:home/arc/Codes/PythonDLCourse/References/RNN/Sequence to sequence learning with neural networks.pdf:pdf},
pages = {1--9},
title = {{Sequence to Sequence Learning with Neural Networks}}
}
