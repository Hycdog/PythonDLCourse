@article{Pang2019,
abstract = {Current state-of-the-art object detection convolutional architectures are manually designed. Although this approach has been successful and delivered strong performances on many benchmarks, their architectures are generally not optimized. For instance, while the backbone models of RetinaNet, and R-CNN family detectors inherit highly optimized architectures from years of research in state-of-art classification networks, their feature pyramid networks which combine features at multiple scales are generally overlooked, and thus under-optimized. Here we aim to learn a better architecture of feature pyramid network for object detection. We search for the best architecture in a form of a directed graph and discover a new feature pyramid network architecture. Our extensive experiments show that the discovered architecture, named NAS-FPN, has many advantages in flexibility and performance. First, NAS-FPN is scalable in that it can be applied repeatedly to improve the performance. Second, not only NAS-FPN works well with the backbone model ResNet-10 that was used during the search process, it also transfers well to other backbone models, such as ResNet-50, ResNet-101 and AmoebaNet. Finally, NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to many state-of-the-art object detection models, including RetinaNet and Mask R-CNN. In particular, our best model achieves 47.5 AP on COCO, which is 2.3 AP better than a single model Mask R-CNN that was pre-trained on much more data. Given the same level of accuracy of 42 AP, our model is 3x faster than Mask R-CNN.},
archivePrefix = {arXiv},
arxivId = {arXiv:1904.07392v1},
author = {Pang, Tsung-yi Lin Ruoming and Le, Quoc V},
eprint = {arXiv:1904.07392v1},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/NAS/NAS-FPN$\backslash$: Learning Scalable Feature Pyramid Architecture for Object Detection.pdf:pdf},
title = {{NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection}},
year = {2019}
}
@article{Wu2019,
abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into different categories. With a focus on graph convolutional networks, we review alternative architectures that have recently been developed; these learning paradigms include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes and benchmarks of the existing algorithms on different learning tasks. Finally, we propose potential research directions in this fast-growing field.},
archivePrefix = {arXiv},
arxivId = {1901.00596},
author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
eprint = {1901.00596},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/A Comprehensive Survey on Graph Neural
Networks.pdf:pdf},
number = {X},
pages = {1--22},
title = {{A Comprehensive Survey on Graph Neural Networks}},
url = {http://arxiv.org/abs/1901.00596},
volume = {X},
year = {2019}
}
@article{Levie2019,
abstract = {The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach, in comparison to other spectral domain convolutional architectures, on spectral image classification, community detection, vertex classification and matrix completion tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.07664v2},
author = {Levie, Ron and Monti, Federico and Bresson, Xavier and Bronstein, Michael M.},
doi = {10.1109/TSP.2018.2879624},
eprint = {arXiv:1705.07664v2},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spectral-based/CayleyNets$\backslash$: Graph Convolutional Neural Networks
with Complex Rational Spectral Filters.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Geometric deep learning,graph convolution neural networks,graph giltering,spectral approaches},
number = {1},
pages = {97--109},
title = {{CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters}},
volume = {67},
year = {2019}
}
@article{Frankle2018,
abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90{\%}, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20{\%} of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
archivePrefix = {arXiv},
arxivId = {1803.03635},
author = {Frankle, Jonathan and Carbin, Michael},
eprint = {1803.03635},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/1803.03635.pdf:pdf},
pages = {1--42},
title = {{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}},
url = {http://arxiv.org/abs/1803.03635},
year = {2018}
}
@article{Shen2018,
abstract = {In general, natural language is governed by a tree structure: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). This is a strict hierarchy: when a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM allows different neurons to track information at different time scales, the architecture does not impose a strict hierarchy. This paper proposes to add such a constraint to the system by ordering the neurons; a vector of "master" input and forget gates ensure that when a given unit is updated, all of the units that follow it in the ordering are also updated. To this end, we propose a new RNN unit: ON-LSTM, which achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.},
archivePrefix = {arXiv},
arxivId = {1810.09536},
author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron},
eprint = {1810.09536},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/ON-LSTM/1810.09536.pdf:pdf},
pages = {1--14},
title = {{Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1810.09536},
year = {2018}
}
@article{Zhang2018,
abstract = {We propose a new network architecture, Gated Attention Networks (GaAN), for learning on graphs. Unlike the traditional multi-head attention mechanism, which equally consumes all attention heads, GaAN uses a convolutional sub-network to control each attention head's importance. We demonstrate the effectiveness of GaAN on the inductive node classification problem. Moreover, with GaAN as a building block, we construct the Graph Gated Recurrent Unit (GGRU) to address the traffic speed forecasting problem. Extensive experiments on three real-world datasets show that our GaAN framework achieves state-of-the-art results on both tasks.},
archivePrefix = {arXiv},
arxivId = {1803.07294},
author = {Zhang, Jiani and Shi, Xingjian and Xie, Junyuan and Ma, Hao and King, Irwin and Yeung, Dit-Yan},
eprint = {1803.07294},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spatial-based/GaAN$\backslash$: Gated Attention Networks for
Learning on Large and Spatiotemporal Graphs.pdf:pdf},
title = {{GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs}},
url = {http://arxiv.org/abs/1803.07294},
year = {2018}
}
@article{Anumanchipalli2018,
abstract = {The ability to read out, or decode, mental content from brain activity has significant practical and scientific implications. For example, technology that translates cortical activity into speech would be transformative for people unable to communicate as a result of neurological impairment. Decoding speech from neural activity is challenging because speaking requires extremely precise and dynamic control of multiple vocal tract articulators on the order of milliseconds. Here, we designed a neural decoder that explicitly leverages the continuous kinematic and sound representations encoded in cortical activity to generate fluent and intelligible speech. A recurrent neural network first decoded vocal tract physiological signals from direct cortical recordings, and then transformed them to acoustic speech output. Robust decoding performance was achieved with as little as 25 minutes of training data. Naive listeners were able to accurately identify these decoded sentences. Additionally, speech decoding was not only effective for audibly produced speech, but also when participants silently mimed speech. These results advance the development of speech neuroprosthetic technology to restore spoken communication in patients with disabling neurological disorders.},
author = {Anumanchipalli, Gopala K and Chartier, Josh and Chang, Edward F},
doi = {10.1101/481267},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Neural Science/Speech synthesis from neural decoding of spoken sentences.pdf:pdf},
issn = {1476-4687},
journal = {bioRxiv},
pages = {481267},
publisher = {Springer US},
title = {{Intelligible speech synthesis from neural decoding of spoken sentences}},
url = {https://www.biorxiv.org/content/early/2018/11/29/481267},
year = {2018}
}
@article{Surasak2018,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. 1},
author = {Surasak, Thattapon and Takahiro, Ito and Cheng, Cheng Hsuan and Wang, Chi En and Sheng, Pao You},
doi = {10.1109/ICBIR.2018.8391187},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/cv/Histograms of Oriented Gradients for Human Detection.pdf:pdf},
isbn = {9781538652541},
journal = {Proceedings of 2018 5th International Conference on Business and Industrial Research: Smart Technology for Next Generation of Information, Engineering, Business and Social Science, ICBIR 2018},
keywords = {Histogram of Oriented Gradients,Human Detection},
pages = {172--176},
title = {{Histogram of oriented gradients for human detection in video}},
year = {2018}
}
@article{Li2018,
abstract = {Graph Convolutional Neural Networks (Graph CNNs) are generalizations of classical CNNs to handle graph data such as molecular data, point could and social networks. Current filters in graph CNNs are built for fixed and shared graph structure. However, for most real data, the graph structures varies in both size and connectivity. The paper proposes a generalized and flexible graph CNN taking data of arbitrary graph structure as input. In that way a task-driven adaptive graph is learned for each graph data while training. To efficiently learn the graph, a distance metric learning is proposed. Extensive experiments on nine graph-structured datasets have demonstrated the superior performance improvement on both convergence speed and predictive accuracy.},
archivePrefix = {arXiv},
arxivId = {1801.03226},
author = {Li, Ruoyu and Wang, Sheng and Zhu, Feiyun and Huang, Junzhou},
eprint = {1801.03226},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spectral-based/Adaptive Graph Convolutional Neural Networks.pdf:pdf},
title = {{Adaptive Graph Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1801.03226},
year = {2018}
}
@article{Dai2018,
abstract = {Many graph analytics problems can be solved via iterative algorithms where the solutions are often characterized by a set of steady-state conditions. Different algorithms respect to different set of fixed point constraints, so instead of using these traditional algorithms, can we learn an algorithm which can obtain the same steady-state solutions automatically from examples, in an effective and scalable way? How to represent the meta learner for such algorithm and how to carry out the learning? In this paper, we propose an embedding representation for iterative algorithms over graphs, and design a learning method which alternates between updating the embeddings and projecting them onto the steady-state constraints. We demonstrate the effectiveness of our framework using a few commonly used graph algorithms, and show that in some cases, the learned algorithm can handle graphs with more than 100,000,000 nodes in a single machine.},
author = {Dai, Hanjun and Kozareva, Zornitsa and Dai, Bo and Smola, Alex and Song, Le},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spatial-based/Learning Steady-States of Iterative Algorithms over Graphs.pdf:pdf},
journal = {Pmlr},
pages = {1106--1114},
title = {{Learning Steady-States of Iterative Algorithms over Graphs}},
url = {http://proceedings.mlr.press/v80/dai2018a.html},
volume = {80},
year = {2018}
}
@article{Gao2018,
abstract = {Convolutional neural networks (CNNs) have achieved great success on grid-like data such as images, but face tremendous challenges in learning from more generic data such as graphs. In CNNs, the trainable local filters enable the automatic extraction of high-level features. The computation with filters requires a fixed number of ordered units in the receptive fields. However, the number of neighboring units is neither fixed nor are they ordered in generic graphs, thereby hindering the applications of convolutional operations. Here, we address these challenges by proposing the learnable graph convolutional layer (LGCL). LGCL automatically selects a fixed number of neighboring nodes for each feature based on value ranking in order to transform graph data into grid-like structures in 1-D format, thereby enabling the use of regular convolutional operations on generic graphs. To enable model training on large-scale graphs, we propose a sub-graph training method to reduce the excessive memory and computational resource requirements suffered by prior methods on graph convolutions. Our experimental results on node classification tasks in both transductive and inductive learning settings demonstrate that our methods can achieve consistently better performance on the Cora, Citeseer, Pubmed citation network, and protein-protein interaction network datasets. Our results also indicate that the proposed methods using sub-graph training strategy are more efficient as compared to prior approaches.},
archivePrefix = {arXiv},
arxivId = {1808.03965},
author = {Gao, Hongyang and Wang, Zhengyang and Ji, Shuiwang},
doi = {10.1145/3219819.3219947},
eprint = {1808.03965},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spatial-based/Large-Scale Learnable Graph Convolutional Networks.pdf:pdf},
isbn = {9781450355520},
keywords = {deep learning,graph convolutional networks,graph mining,large-},
title = {{Large-Scale Learnable Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1808.03965{\%}0Ahttp://dx.doi.org/10.1145/3219819.3219947},
year = {2018}
}
@phdthesis{Takashima2018,
abstract = {1. The electrical activity of up to eight concurrently active motor units has been recorded from the human deltoid and first dorsal interosseous (f.d.i.) muscles. The detected myoelectric signals have been decomposed into their constituent motor-unit action potential trains using a recently developed technique. 2. Concurrently active motor unit behaviour has been examined during triangular force-varying isometric contractions reaching 40 and 80{\%} of maximal voluntary contraction (m.v.c.). Experiments were performed on four normal subjects and three groups of highly trained performers (long-distance swimmers, powerlifters and pianists). 3. Results revealed a highly ordered recruitment and decruitment scheme, based on motoneurone excitability, in both muscles and in all subject groups. 4. Differences were observed between the initial (recruitment) and final (decruitment) firing rates in each muscle. These parameters were invariant with respect to the force rates studied, although some differences were observed among subject groups. 5. In general, firing rates of f.d.i. motor units increased steadily with increasing force (up to 80{\%} m.v.c.). The firing rates of deltoid motor units rose sharply just after recruitment and then increased only slightly thereafter. 6. Recruitment was found to be the major mechanism for generating extra force between 40 and 80{\%} m.v.c. in the deltoid, while rate coding played the major role in the f.d.i. 7. The potential of rate coding for increasing force levels up to m.v.c. is discussed.},
author = {Takashima, Fumiki and Mizunoya, Kazuyuki and Morimoto, Yuji},
booktitle = {Japanese Journal of Anesthesiology},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/MTCNN/Robust real-time face detection.pdf:pdf},
issn = {00214892},
keywords = {Cross-lock,Non-closed arterial line system,T-shape stopcock},
number = {2},
pages = {203--207},
title = {{Robust Real-time Object Detection}},
volume = {67},
year = {2018}
}
@article{Li2018a,
abstract = {Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). Motivated by online Passive-Agressive (PA) algorithm, we introduce the temporal regularization to SRDCF with single sample, thus resulting in our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Experiments are conducted on three benchmark datasets: OTB-2015, Temple-Color, and VOT-2016. Compared with SRDCF, STRCF with hand-crafted features provides a 5 times speedup and achieves a gain of 5.4{\%} and 3.6{\%} AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF combined with CNN features also performs favorably against state-of-the-art CNN-based trackers and achieves an AUC score of 68.3{\%} on OTB-2015.},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.08679v1},
author = {Li, Feng and Tian, Cheng and Zuo, Wangmeng and Zhang, Lei and Yang, Ming Hsuan},
doi = {10.1109/CVPR.2018.00515},
eprint = {arXiv:1803.08679v1},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/STRCF/Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4904--4913},
title = {{Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking}},
year = {2018}
}
@article{Battaglia2018,
abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
archivePrefix = {arXiv},
arxivId = {1806.01261},
author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
eprint = {1806.01261},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/Relational inductive biases, deep learning, and graph networks.pdf:pdf},
pages = {1--40},
title = {{Relational inductive biases, deep learning, and graph networks}},
url = {http://arxiv.org/abs/1806.01261},
year = {2018}
}
@article{Zoph2018,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4{\%} error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.07012v4},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
doi = {10.1109/CVPR.2018.00907},
eprint = {arXiv:1707.07012v4},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/NAS/Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8697--8710},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
year = {2018}
}
@article{Monti2017,
abstract = {Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.08402v3},
author = {Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodol{\`{a}}, Emanuele and Svoboda, Jan and Bronstein, Michael M.},
doi = {10.1109/CVPR.2017.576},
eprint = {arXiv:1611.08402v3},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spatial-based/Geometric deep learning on graphs and manifolds using mixture model CNNs.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {5425--5434},
title = {{Geometric deep learning on graphs and manifolds using mixture model CNNs}},
volume = {2017-Janua},
year = {2017}
}
@article{Velickovic2017,
abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
archivePrefix = {arXiv},
arxivId = {1710.10903},
author = {Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
eprint = {1710.10903},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spatial-based/GRAPH ATTENTION NETWORKS.pdf:pdf},
pages = {1--12},
title = {{Graph Attention Networks}},
url = {http://arxiv.org/abs/1710.10903},
year = {2017}
}
@article{Moravcik2017,
abstract = {Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.01724v1},
author = {Moravcik, Matej and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
doi = {10.1126/science.aam6960.1},
eprint = {arXiv:1701.01724v1},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Game/DeepStack$\backslash$: Expert-Level Artificial Intelligence in
Heads-Up No-Limit Poker.pdf:pdf},
journal = {Science},
number = {May},
pages = {1--32},
title = {{DeepStack : Expert-Level Artificial Intelligence in No-Limit Poker}},
url = {http://science.sciencemag.org/},
volume = {513},
year = {2017}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.06870v3},
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
doi = {10.1109/ICCV.2017.322},
eprint = {arXiv:1703.06870v3},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/R-CNN/Mask R-CNN.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2980--2988},
title = {{Mask R-CNN}},
volume = {2017-Octob},
year = {2017}
}
@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v3},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {arXiv:1506.01497v3},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/R-CNN/Faster R-CNN$\backslash$: Towards Real-Time Object
Detection with Region Proposal Networks.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27295650},
volume = {39},
year = {2017}
}
@article{Chen2017,
abstract = {It is a usual practice to ignore any structural information underlying classes in multi-class classification. In this paper, we propose a graph convolutional network (GCN) augmented neural network classifier to exploit a known, underlying graph structure of labels. The proposed approach resembles an (approximate) inference procedure in, for instance, a conditional random field (CRF). We evaluate the proposed approach on document classification and object recognition and report both accuracies and graph-theoretic metrics that correspond to the consistency of the model's prediction. The experiment results reveal that the proposed model outperforms a baseline method which ignores the graph structures of a label space in terms of graph-theoretic metrics.},
archivePrefix = {arXiv},
arxivId = {1710.04908},
author = {Chen, Meihao and Lin, Zhuoru and Cho, Kyunghyun},
eprint = {1710.04908},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/Graph Convolutional Networks for Classification
with a Structured Label Space.pdf:pdf},
title = {{Graph Convolutional Networks for Classification with a Structured Label Space}},
url = {http://arxiv.org/abs/1710.04908},
year = {2017}
}
@article{Hamilton2017,
abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
archivePrefix = {arXiv},
arxivId = {1706.02216},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
eprint = {1706.02216},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spatial-based/Inductive Representation Learning on Large Graphs.pdf:pdf},
number = {Nips},
pages = {1--19},
title = {{Inductive Representation Learning on Large Graphs}},
url = {http://arxiv.org/abs/1706.02216},
year = {2017}
}
@article{Molchanov2017,
abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
archivePrefix = {arXiv},
arxivId = {1701.05369},
author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
eprint = {1701.05369},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
pages = {249--256},
title = {{Variational Dropout Sparsifies Deep Neural Networks}},
url = {http://arxiv.org/abs/1701.05369},
volume = {9},
year = {2017}
}
@article{Purkait2017,
abstract = {Image based localization is one of the important problems in computer vision due to its wide applicability in robotics, augmented reality, and autonomous systems. There is a rich set of methods described in the literature how to geometrically register a 2D image w.r.t.{\$}\backslash{\$} a 3D model. Recently, methods based on deep (and convolutional) feedforward networks (CNNs) became popular for pose regression. However, these CNN-based methods are still less accurate than geometry based methods despite being fast and memory efficient. In this work we design a deep neural network architecture based on sparse feature descriptors to estimate the absolute pose of an image. Our choice of using sparse feature descriptors has two major advantages: first, our network is significantly smaller than the CNNs proposed in the literature for this task---thereby making our approach more efficient and scalable. Second---and more importantly---, usage of sparse features allows to augment the training data with synthetic viewpoints, which leads to substantial improvements in the generalization performance to unseen poses. Thus, our proposed method aims to combine the best of the two worlds---feature-based localization and CNN-based pose regression--to achieve state-of-the-art performance in the absolute pose estimation. A detailed analysis of the proposed architecture and a rigorous evaluation on the existing datasets are provided to support our method.},
archivePrefix = {arXiv},
arxivId = {1712.03452},
author = {Purkait, Pulak and Zhao, Cheng and Zach, Christopher},
eprint = {1712.03452},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/SPP/SPP-Net$\backslash$: Deep Absolute Pose Regression with Synthetic Views.pdf:pdf},
title = {{SPP-Net: Deep Absolute Pose Regression with Synthetic Views}},
url = {http://arxiv.org/abs/1712.03452},
year = {2017}
}
@article{Lin2017,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.03144v2},
author = {Lin, Tsung Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
doi = {10.1109/CVPR.2017.106},
eprint = {arXiv:1612.03144v2},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/FPN/Feature Pyramid Networks for Object Detection.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {936--944},
title = {{Feature pyramid networks for object detection}},
volume = {2017-Janua},
year = {2017}
}
@article{Lin2017a,
abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.02002v2},
author = {Lin, Tsung Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
doi = {10.1109/ICCV.2017.324},
eprint = {arXiv:1708.02002v2},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/RetinaNet/Focal Loss for Dense Object Detection.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2999--3007},
title = {{Focal Loss for Dense Object Detection}},
volume = {2017-Octob},
year = {2017}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
eprint = {1602.07261},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Inception-v4, Inception-ResNet and
the Impact of Residual Connections on Learning.pdf:pdf},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{Kipf2016,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N. and Welling, Max},
eprint = {1609.02907},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spectral-based/SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS.pdf:pdf},
pages = {1--14},
title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
url = {http://arxiv.org/abs/1609.02907},
year = {2016}
}
@article{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.02325v5},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {arXiv:1512.02325v5},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/SSD/SSD$\backslash$: Single Shot MultiBox Detector.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
pages = {21--37},
title = {{SSD: Single shot multibox detector}},
volume = {9905 LNCS},
year = {2016}
}
@article{Defferrard2016,
abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
archivePrefix = {arXiv},
arxivId = {1606.09375},
author = {Defferrard, Micha{\"{e}}l and Bresson, Xavier and Vandergheynst, Pierre},
eprint = {1606.09375},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spectral-based/Convolutional Neural Networks on Graphs
with Fast Localized Spectral Filtering.pdf:pdf},
number = {Nips},
title = {{Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}},
url = {http://arxiv.org/abs/1606.09375},
year = {2016}
}
@article{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.05027v3},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {arXiv:1603.05027v3},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Identity Mappings in Deep Residual Networks.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
volume = {9908 LNCS},
year = {2016}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Game/Mastering the game of Go with deep
neural networks and tree search.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26819042},
volume = {529},
year = {2016}
}
@article{AIResearch.com2016,
abstract = {Speech Network Image},
author = {{AI Research (.com)}},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/RNN/Deep Neural Networks for Acoustic Modeling
in Speech Recognition.pdf:pdf},
journal = {Http://Airesearch.Com},
pages = {1--27},
title = {{Deep Neural Networks for Acoustic Modeling in Speech Recognition – AI Research}},
url = {http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speech-recognition/},
year = {2016}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/An overview of gradient descent optimizationalgorithms.pdf:pdf},
pages = {1--14},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@article{Niepert2016,
abstract = {Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.},
archivePrefix = {arXiv},
arxivId = {1605.05273},
author = {Niepert, Mathias and Ahmed, Mohamed and Kutzkov, Konstantin},
eprint = {1605.05273},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spatial-based/Learning Convolutional Neural Networks for Graphs.pdf:pdf},
title = {{Learning Convolutional Neural Networks for Graphs}},
url = {http://arxiv.org/abs/1605.05273},
volume = {1},
year = {2016}
}
@article{Zhang2016,
abstract = {Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance.},
author = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
doi = {10.1109/LSP.2016.2603342},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/MTCNN/Joint Face Detection and Alignment using
Multi-task Cascaded Convolutional Networks.pdf:pdf},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Cascaded convolutional neural network (CNN),face alignment,face detection},
number = {10},
pages = {1499--1503},
title = {{Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks}},
volume = {23},
year = {2016}
}
@article{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V.},
eprint = {1611.01578},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/NAS/NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING.pdf:pdf},
pages = {1--16},
title = {{Neural Architecture Search with Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.01578},
year = {2016}
}
@article{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.4842v1},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {arXiv:1409.4842v1},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Going deeper with convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{He2015a,
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224 × 224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 × faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank {\#}2 in object detection and {\#}3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.4729v4},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/TPAMI.2015.2389824},
eprint = {arXiv:1406.4729v4},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/SPP/Spatial pyramid pooling
in deep convolutional networks for visual recognition.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Neural Networks,Image Classification,Object Detection,Spatial Pyramid Pooling},
number = {9},
pages = {1904--1916},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
volume = {37},
year = {2015}
}
@article{Szegedy2015a,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
eprint = {1512.00567},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Henaff2015,
abstract = {Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.},
archivePrefix = {arXiv},
arxivId = {1506.05163},
author = {Henaff, Mikael and Bruna, Joan and LeCun, Yann},
eprint = {1506.05163},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spectral-based/Deep Convolutional Networks on Graph-Structured Data.pdf:pdf},
pages = {1--10},
title = {{Deep Convolutional Networks on Graph-Structured Data}},
url = {http://arxiv.org/abs/1506.05163},
year = {2015}
}
@article{Kuo2015,
abstract = {Existing object proposal approaches use primarily bottom-up cues to rank proposals, while we believe that objectness is in fact a high level construct. We argue for a data-driven, semantic approach for ranking object proposals. Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method. We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster. We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP. Our implementation achieves this performance while running at 260 ms per image.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.02146v2},
author = {Kuo, Weicheng and Hariharan, Bharath and Malik, Jitendra},
doi = {10.1109/ICCV.2015.285},
eprint = {arXiv:1505.02146v2},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/cv/DeepBox$\backslash$: Learning Objectness with Convolutional Networks.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2479--2487},
title = {{DeepBox: Learning objectness with convolutional networks}},
volume = {2015 Inter},
year = {2015}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/Batch Normalization$\backslash$: Accelerating Deep Network Training by
Reducing Internal Covariate Shift.pdf:pdf},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
eprint = {1506.02640},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/You Only Look Once$\backslash$:
Unified, Real-Time Object Detection.pdf:pdf},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Game/Human-level control through deep reinforcement
learning.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25719670},
volume = {518},
year = {2015}
}
@article{Shi2015,
abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
archivePrefix = {arXiv},
arxivId = {1506.04214},
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
eprint = {1506.04214},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Convolutional LSTM Network$\backslash$: A Machine Learning
Approach for Precipitation Nowcasting.pdf:pdf},
pages = {1--12},
title = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
url = {http://arxiv.org/abs/1506.04214},
year = {2015}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {arXiv:1504.08083v2},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {arXiv:1504.08083v2},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/R-CNN/Fast R-CNN.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1440--1448},
title = {{Fast R-CNN}},
volume = {2015 Inter},
year = {2015}
}
@article{Lipton2015,
abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
archivePrefix = {arXiv},
arxivId = {1506.00019},
author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
eprint = {1506.00019},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/RNN/A Critical Review of Recurrent Neural Networks
for Sequence Learning.pdf:pdf},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning}},
url = {http://arxiv.org/abs/1506.00019},
year = {2015}
}
@article{Tang2015,
abstract = {This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the "LINE," which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online.},
archivePrefix = {arXiv},
arxivId = {1503.03578},
author = {Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
doi = {10.1145/2736277.2741093},
eprint = {1503.03578},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/LINE$\backslash$: Large-scale Information Network Embedding.pdf:pdf},
isbn = {9781450334693},
keywords = {dimension reduction,feature learn-,information network embedding,ing,scalability},
title = {{LINE: Large-scale Information Network Embedding}},
url = {http://arxiv.org/abs/1503.03578{\%}0Ahttp://dx.doi.org/10.1145/2736277.2741093},
year = {2015}
}
@article{Yang2015,
abstract = {In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99{\%} on the challenging FDDB benchmark, outperforming the state-of-the-art method by a large margin of 2.91{\%}. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06451v1},
author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
doi = {10.1109/ICCV.2015.419},
eprint = {arXiv:1509.06451v1},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/MTCNN/From Facial Parts Responses to Face Detection$\backslash$: A Deep Learning Approach.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {3},
pages = {3676--3684},
title = {{From facial parts responses to face detection: A deep learning approach}},
volume = {2015 Inter},
year = {2015}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Deep Residual Learning For Image Recognition.pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1311.2901v3},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Visualizing and Understanding Convolutional Networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
volume = {8689 LNCS},
year = {2014}
}
@article{Zitnick2014,
abstract = {The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box's boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96{\%} object recall at overlap threshold of 0.5 and over 75{\%} recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
author = {Zitnick, C. Lawrence and Doll{\'{a}}r, Piotr},
doi = {10.1007/978-3-319-10602-1_26},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/cv/Edge Boxes$\backslash$: Locating Object Proposals from Edges.pdf:pdf},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {edge detection,object detection,object proposals},
number = {PART 5},
pages = {391--405},
title = {{Edge boxes: Locating object proposals from edges}},
volume = {8693 LNCS},
year = {2014}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {1406.2661},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GAN/Generative Adversarial Nets.pdf:pdf},
pages = {1--9},
title = {{Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
eprint = {1409.3215},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/RNN/Sequence to sequence learning with neural networks.pdf:pdf},
pages = {1--9},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@article{Hadidi2014,
abstract = {Deep Neural Networks (DNNs) have recently shown outstanding performance on the task of whole image classification. In this paper we go one step further and address the problem of object detection -- not only classifying but also precisely localizing objects of various classes using DNNs. We present a simple and yet powerful formulation of object detection as a regression to object masks. We define a multi-scale inference procedure which is able to produce a high-resolution object detection at a low cost by a few network applications. The approach achieves state-of-the-art performance on Pascal 2007 VOC.},
author = {Hadidi, Niloufar Niakosari and Cullen, Kathryn R. and Hall, Leah M.J. and Lindquist, Ruth and Buckwalter, Kathleen C. and Mathews, Emily},
doi = {10.3928/19404921-20140820-01},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/cv/deep-neural-networks-for-object-detection.pdf:pdf},
issn = {1940-4921},
journal = {Research in Gerontological Nursing},
number = {5},
pages = {200--205},
title = {{Functional Magnetic Resonance Imaging as Experienced by Stroke Survivors}},
volume = {7},
year = {2014}
}
@article{Denton2014,
abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1{\%} of the original model.},
archivePrefix = {arXiv},
arxivId = {1404.0736},
author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
eprint = {1404.0736},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/R-CNN/Exploiting Linear Structure Within Convolutional
Networks for Efficient Evaluation.pdf:pdf},
pages = {1--11},
title = {{Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation}},
url = {http://arxiv.org/abs/1404.0736},
year = {2014}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Very Deep Convolutional Networks For Large-Scale Image Recognition.pdf:pdf},
pages = {1--14},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Girshick2014,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2524v5},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {arXiv:1311.2524v5},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/R-CNN/Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
eprint = {1409.2329},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/RNN/Recurrent Neural Network Regularization.pdf:pdf},
number = {2013},
pages = {1--8},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/Network In Network.pdf:pdf},
pages = {1--10},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/OverFeat$\backslash$:Integrated Recognition, Localization and Detectionusing Convolutional Networks.pdf:pdf},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@article{Erhan2013,
abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
archivePrefix = {arXiv},
arxivId = {1312.2249},
author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
eprint = {1312.2249},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/Scalable Object Detection using Deep Neural Networks.pdf:pdf},
title = {{Scalable Object Detection using Deep Neural Networks}},
url = {http://arxiv.org/abs/1312.2249},
year = {2013}
}
@article{Bruna2013,
abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
archivePrefix = {arXiv},
arxivId = {1312.6203},
author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
eprint = {1312.6203},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/spectral-based/Spectral Networks and Deep Locally Connected Networks on Graphs.pdf:pdf},
pages = {1--14},
title = {{Spectral Networks and Locally Connected Networks on Graphs}},
url = {http://arxiv.org/abs/1312.6203},
year = {2013}
}
@article{Sande2013,
abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce Selective Search which combines the strength of both an exhaustive search and seg-mentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our Selective Search results in a small set of data-driven, class-independent, high quality locations, yielding 99{\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The Selective Search software is made publicly available 1 .},
author = {Sande, Koen Van De},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/cv/Selective search for object recognition.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {()},
title = {{Selective Search for Object Localisation}},
url = {http://disi.unitn.it/},
year = {2013}
}
@article{Krauss2012,
abstract = {CXCR4 is a G-protein coupled receptor for CXCL12 that plays an important role in human immunodeficiency virus infection, cancer growth and metastasization, immune cell trafficking and WHIM syndrome. In the absence of an X-ray crystal structure, theoretical modeling of the CXCR4 receptor remains an important tool for structure-function analysis and to guide the discovery of new antagonists with potential clinical use. In this study, the combination of experimental data and molecular modeling approaches allowed the development of optimized ligand-receptor models useful for elucidation of the molecular determinants of small molecule binding and functional antagonism. The ligand-guided homology modeling approach used in this study explicitly re-shaped the CXCR4 binding pocket in order to improve discrimination between known CXCR4 antagonists and random decoys. Refinement based on multiple test-sets with small compounds from single chemotypes provided the best early enrichment performance. These results provide an important tool for structure-based drug design and virtual ligand screening of new CXCR4 antagonists.},
author = {Krauss, Ronald M. and Nichols, Alex V.},
doi = {10.1007/978-1-4684-1262-8_2},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/MTCNN/A Convolutional Neural Network Cascade for Face Detection.pdf:pdf},
journal = {Lipoprotein Deficiency Syndromes},
pages = {17--27},
title = {{A Convolutional Neural Network Cascade for Face Detection}},
year = {2012}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/CNN/ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{2012 AlexNet}},
year = {2012}
}
@article{Arbelaez2011,
abstract = {This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.},
author = {Arbel{\'{a}}ez, Pablo and Maire, Michael and Fowlkes, Charless and Malik, Jitendra},
doi = {10.1109/TPAMI.2010.161},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/cv/Contour Detection and
Hierarchical Image Segmentation.pdf:pdf;:home/hu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arbel{\'{a}}ez et al. - 2011 - Contour detection and hierarchical image segmentation.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {5},
pages = {898--916},
pmid = {20733228},
title = {{Contour detection and hierarchical image segmentation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20733228},
volume = {33},
year = {2011}
}
@article{Dhillon2007,
abstract = {A variety of clustering algorithms have recently been proposed to handle data that is not linearly separable; spectral clustering and kernel k-means are two of the main methods. In this paper, we discuss an equivalence between the objective functions used in these seemingly different methods--in particular, a general weighted kernel k-means objective is mathematically equivalent to a weighted graph clustering objective. We exploit this equivalence to develop a fast, high-quality multilevel algorithm that directly optimizes various weighted graph clustering objectives, such as the popular ratio cut, normalized cut, and ratio association criteria. This eliminates the need for any eigenvector computation for graph clustering problems, which can be prohibitive for very large graphs. Previous multilevel graph partitioning methods, such as Metis, have suffered from the restriction of equal-sized clusters; our multilevel algorithm removes this restriction by using kernel k-means to optimize weighted graph cuts. Experimental results show that our multilevel algorithm outperforms a state-of-the-art spectral clustering algorithm in terms of speed, memory usage, and quality. We demonstrate that our algorithm is applicable to large-scale clustering tasks such as image segmentation, social network analysis and gene network analysis.},
author = {Dhillon, Inderjit S. and Guan, Yuqiang and Kulis, Brian},
doi = {10.1109/TPAMI.2007.1115},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/clustering/Weighted Graph Cuts without Eigenvectors$\backslash$: A Multilevel Approach.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Clustering,Data mining,Graph partitioning,K-means,Kernel,Segmentation,Spectral clustering,k-means},
number = {11},
pages = {1944--1957},
title = {{Weighted graph cuts without eigenvectors a multilevel approach}},
volume = {29},
year = {2007}
}
@article{VonLuxburg2007,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {{Von Luxburg}, Ulrike},
doi = {10.1007/s11222-007-9033-z},
eprint = {arXiv:0711.0189v1},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/clustering/A Tutorial on Spectral Clustering.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Graph Laplacian,Spectral clustering},
number = {4},
pages = {395--416},
title = {{A tutorial on spectral clustering}},
volume = {17},
year = {2007}
}
@article{Kushnir2006,
abstract = {We present a novel multiscale clustering algorithm inspired by algebraic multigrid techniques. Our method begins with assembling data points according to local similarities. It uses an aggregation process to obtain reliable scale-dependent global properties, which arise from the local similarities. As the aggregation process proceeds, these global properties affect the formation of coherent clusters. The global features that can be utilized are for example density, shape, intrinsic dimensionality and orientation. The last three features are a part of the manifold identification process which is performed in parallel to the clustering process. The algorithm detects clusters that are distinguished by their multiscale nature, separates between clusters with different densities, and identifies and resolves intersections between clusters. The algorithm is tested on synthetic and real data sets, its running time complexity is linear in the size of the data set. {\textcopyright} 2006 Pattern Recognition Society.},
author = {Kushnir, Dan and Galun, Meirav and Brandt, Achi},
doi = {10.1016/j.patcog.2006.04.007},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/clustering/Fast multiscale clustering and manifold identification.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Aggregation,Algebraic multigrid (AMG),Astrophysical models,Data analysis,Graph partitioning,Manifold,Similarity-based clustering},
number = {10},
pages = {1876--1891},
title = {{Fast multiscale clustering and manifold identification}},
volume = {39},
year = {2006}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of affine distortion , change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/cv/Distinctive image features from scale-invariant keypoints.pdf:pdf},
journal = {International Journal of Computer Vision},
pages = {1--28},
title = {{Distinctive Image Features from Scale-Invariant Keypoints
.pdf}},
year = {2004}
}
@article{,
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
journal = {proc. OF THE IEEE},
title = {{Gradient-based learning applied to document recognition}},
url = {http://ieeexplore.ieee.org/document/726791/{\#}full-text-section},
year = {1998}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/RNN/lstm.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Karypis1995,
abstract = {partition algorithm},
author = {Karypis, George and Kumar, Vipin},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/GraphDL/clustering/METIS  Unstructured Graph Partitioning and Sparse Matrix Ordering System.pdf:pdf},
journal = {Unstructured Graph Partinioning and Sparse Matrix Ordering},
pages = {1--16},
title = {{Metis: Unstructured Graph Partitioning and Sparse Matrix Ordering}},
year = {1995}
}
@misc{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.},
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
booktitle = {Nature},
doi = {10.1038/323533a0},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/backprop{\_}old.pdf:pdf},
pages = {533--536},
title = {{Backprop{\_}Old.Pdf}},
volume = {323},
year = {1986}
}
@misc{,
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/Backpropagation Applied to Handwritten Zip Code Recognition.pdf:pdf},
title = {{【LeNet-5】.pdf}}
}
@article{Dean,
archivePrefix = {arXiv},
arxivId = {fa},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
doi = {10.1109/ICDAR.2011.95},
eprint = {fa},
file = {:home/hu/PycharmProjects/PythonDLCourse/References/Misc/Large scale distributed deep networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {1--11},
pmid = {43479959},
title = {{Large Scale Distributed Deep Networks}}
}
